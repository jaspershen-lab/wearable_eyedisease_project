library(tidyverse)
library(rstanarm)
library(bayesplot)
library(ggplot2)
library(gridExtra)
library(r4projects)
library(pROC)
library(loo)

# è®¾ç½®è´å¶æ–¯åˆ†æé€‰é¡¹
options(mc.cores = parallel::detectCores())

# Set working directory
setwd(get_project_wd())
rm(list = ls())

# ================== 1. æ•°æ®å‡†å¤‡ ==================

cat("===== å¯ç©¿æˆ´è®¾å¤‡æŒ‡æ ‡é¢„æµ‹OCTAé¢„ååˆ†æ - åŒæ¨¡å‹ç­–ç•¥ =====\n")

# åŠ è½½æ•°æ®æ–‡ä»¶
raw_wearable_file <- "3_data_analysis/6_clustering_modeling/data_prepare/1m/mfuzz_D_Surg1_8h_filtered.csv"
wearable_cluster_file <- "3_data_analysis/6_clustering_modeling/time_window_clustering/late_recovery_detailed_membership_fixed.csv"
# outcome_file <- "3_data_analysis/6_clustering_modeling/mfuzz/comprehensive_cluster/ppv_WF_cluster_results.csv"
outcome_file <- "3_data_analysis/6_clustering_modeling/mfuzz/WF_only_cluster/ppv_comprehensive_cluster_results.csv"
baseline_info <- read.csv("2_data/analysis_data/baseline_info.csv", stringsAsFactors = FALSE)

# å®‰å…¨åŠ è½½æ•°æ®å‡½æ•°
load_data_safely <- function(file_path, data_name) {
  if(file.exists(file_path)) {
    data <- read.csv(file_path, stringsAsFactors = FALSE)
    cat(sprintf("âœ“ æˆåŠŸåŠ è½½ %s: %d è¡Œæ•°æ®\n", data_name, nrow(data)))
    return(data)
  } else {
    cat(sprintf("âŒ æ–‡ä»¶ä¸å­˜åœ¨: %s\n", file_path))
    stop(sprintf("å¿…éœ€çš„æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: %s", file_path))
  }
}

# åŠ è½½æ•°æ®
raw_wearable_data <- load_data_safely(raw_wearable_file, "åŸå§‹å¯ç©¿æˆ´è®¾å¤‡æ•°æ®")
wearable_cluster_data <- load_data_safely(wearable_cluster_file, "å¯ç©¿æˆ´è®¾å¤‡èšç±»ç»“æœ")
outcome_data <- load_data_safely(outcome_file, "OCTAé¢„åæ•°æ®")

# æå–Late Recoveryæ—¶é—´çª—å£çš„æŒ‡æ ‡å‡½æ•°
extract_late_recovery_metrics <- function(raw_data) {
  late_recovery_days <- 16:30
  
  cv_rhr_cols <- c()
  steps_max_cols <- c()
  
  for(day in late_recovery_days) {
    cv_rhr_col <- paste0("day_", day, "_cv_rhr_1")
    steps_max_col <- paste0("day_", day, "_steps_max")
    
    if(cv_rhr_col %in% colnames(raw_data)) {
      cv_rhr_cols <- c(cv_rhr_cols, cv_rhr_col)
    }
    if(steps_max_col %in% colnames(raw_data)) {
      steps_max_cols <- c(steps_max_cols, steps_max_col)
    }
  }
  
  cat("æ‰¾åˆ°çš„Late Recovery CV RHRåˆ—:", length(cv_rhr_cols), "ä¸ª\n")
  cat("æ‰¾åˆ°çš„Late Recovery Steps Maxåˆ—:", length(steps_max_cols), "ä¸ª\n")
  
  result_data <- data.frame(subject_id = raw_data$subject_id)
  
  # è®¡ç®—Late RecoveryæœŸé—´çš„å¹³å‡å€¼
  cv_rhr_data <- raw_data[, cv_rhr_cols, drop = FALSE]
  result_data$late_recovery_cv_rhr_1 <- rowMeans(cv_rhr_data, na.rm = TRUE)
  
  steps_max_data <- raw_data[, steps_max_cols, drop = FALSE]
  result_data$late_recovery_steps_max <- rowMeans(steps_max_data, na.rm = TRUE)
  
  return(result_data)
}

wearable_metrics <- extract_late_recovery_metrics(raw_wearable_data)

# è®¾ç½®è¾“å‡ºç›®å½•
output_dir <- "3_data_analysis/6_clustering_modeling/late_recovery_subanalysis/two_model_bayesian_analysis"
dir.create(output_dir, recursive = TRUE, showWarnings = FALSE)
setwd(output_dir)

# ç»Ÿä¸€IDåˆ—å
standardize_id_column <- function(data) {
  if("subject_id" %in% names(data)) {
    return(data)
  } else if("ID" %in% names(data)) {
    names(data)[names(data) == "ID"] <- "subject_id"
    return(data)
  } else {
    stop("æ‰¾ä¸åˆ°IDåˆ— (subject_id æˆ– ID)")
  }
}

wearable_metrics <- standardize_id_column(wearable_metrics)
wearable_cluster_data <- standardize_id_column(wearable_cluster_data)
outcome_data <- standardize_id_column(outcome_data)

# åˆå¹¶æ•°æ®
wearable_combined <- merge(wearable_metrics, wearable_cluster_data, 
                           by = "subject_id", suffixes = c("", "_cluster"))

prediction_data <- merge(wearable_combined, outcome_data, 
                         by = "subject_id", suffixes = c("_wearable", "_outcome"))

# åˆ›å»ºäºŒåˆ†ç±»ç›®æ ‡å˜é‡
prediction_data$good_outcome <- ifelse(prediction_data$max_cluster_outcome == 2, 1, 0)
prediction_data$outcome_label <- factor(prediction_data$good_outcome, 
                                        levels = c(0, 1), 
                                        labels = c("Poor Outcome", "Good Outcome"))

# åˆ›å»ºç‰¹å¾æ•°æ®æ¡†
features_data <- data.frame(
  subject_id = prediction_data$subject_id,
  cv_rhr = prediction_data$late_recovery_cv_rhr_1,
  steps_max = prediction_data$late_recovery_steps_max,
  wearable_cluster = prediction_data$max_cluster_wearable,
  outcome = prediction_data$outcome_label,
  good_outcome = prediction_data$good_outcome
)

# å¤„ç†ç¼ºå¤±å€¼
initial_n <- nrow(features_data)
features_data <- features_data[complete.cases(features_data[, c("cv_rhr", "steps_max", "good_outcome")]), ]
final_n <- nrow(features_data)

cat("æ ·æœ¬å¤„ç†:\n")
cat("- åˆå§‹æ ·æœ¬æ•°:", initial_n, "\n")
cat("- å®Œæ•´æ¡ˆä¾‹æ•°:", final_n, "\n")

# ================== 2. æ•´åˆä¸´åºŠå˜é‡ï¼ˆä»…å¹´é¾„å’Œæ€§åˆ«ï¼‰==================

cat("\n===== æ•´åˆä¸´åºŠå˜é‡ (å¹´é¾„ + æ€§åˆ«) =====\n")

# æ•´åˆä¸´åºŠåŸºçº¿ä¿¡æ¯
if(ncol(baseline_info) >= 2) {
  colnames(baseline_info)[2] <- "subject_id"
}

if("subject_id" %in% names(baseline_info)) {
  features_data <- features_data %>%
    left_join(baseline_info %>% dplyr::select(subject_id, age, gender), 
              by = "subject_id")
  
  # æ•°æ®æ¸…ç†å‡½æ•°ï¼šå¤„ç† "." ä½œä¸ºç¼ºå¤±å€¼çš„æƒ…å†µ
  clean_numeric_column <- function(x) {
    x[x == "." | x == "" | is.na(x)] <- NA
    as.numeric(x)
  }
  
  # æ¸…ç†ä¸´åºŠå˜é‡
  features_data <- features_data %>%
    mutate(
      age = clean_numeric_column(age),
      gender = clean_numeric_column(gender)
    )
  
  # æ£€æŸ¥ä¸´åºŠå˜é‡å®Œæ•´æ€§
  clinical_completeness <- features_data %>%
    summarise(
      age_complete = sum(!is.na(age)),
      gender_complete = sum(!is.na(gender)),
      total_n = n()
    )
  
  cat("ä¸´åºŠå˜é‡å®Œæ•´æ€§:\n")
  cat("- Age:", clinical_completeness$age_complete, "/", clinical_completeness$total_n, "\n")
  cat("- Gender:", clinical_completeness$gender_complete, "/", clinical_completeness$total_n, "\n")
  
  # å†³å®šæ˜¯å¦çº³å…¥ä¸´åºŠå˜é‡
  use_clinical <- (clinical_completeness$age_complete >= final_n * 0.7 && 
                     clinical_completeness$gender_complete >= final_n * 0.7)
  
  cat("åˆ†æç­–ç•¥:\n")
  cat("- ä½¿ç”¨ä¸´åºŠå˜é‡ (Age + Gender):", ifelse(use_clinical, "æ˜¯", "å¦"), "\n")
  cat("- æ’é™¤HbA1c: æ˜¯ (é¿å…å°æ ·æœ¬å¼‚å¸¸å…³è”)\n")
  
} else {
  use_clinical <- FALSE
  cat("âš ï¸ æœªæ‰¾åˆ°ä¸´åºŠåŸºçº¿ä¿¡æ¯ï¼Œä»…ä½¿ç”¨å¯ç©¿æˆ´è®¾å¤‡æ•°æ®\n")
}

# ================== 3. æ•°æ®æ ‡å‡†åŒ– ==================

cat("\n===== æ•°æ®æ ‡å‡†åŒ– =====\n")

# ä¸ºè´å¶æ–¯åˆ†ææ ‡å‡†åŒ–è¿ç»­å˜é‡
features_data_scaled <- features_data %>%
  mutate(
    cv_rhr_scaled = as.numeric(scale(cv_rhr)),
    steps_max_scaled = as.numeric(scale(steps_max))
  )

if(use_clinical) {
  features_data_scaled <- features_data_scaled %>%
    mutate(
      age_scaled = if(!all(is.na(age))) as.numeric(scale(age)) else NA_real_,
      gender = factor(gender, levels = c(0, 1), labels = c("Female", "Male"))
    )
}

cat("æ ‡å‡†åŒ–å®Œæˆ:\n")
cat("- CV RHRæ ‡å‡†åŒ–: å‡å€¼=0, æ ‡å‡†å·®=1\n")
cat("- Steps Maxæ ‡å‡†åŒ–: å‡å€¼=0, æ ‡å‡†å·®=1\n")
if(use_clinical) {
  cat("- Ageæ ‡å‡†åŒ–: å‡å€¼=0, æ ‡å‡†å·®=1\n")
  cat("- Genderå› å­åŒ–: Female/Male\n")
}

# ================== 4. åŒæ¨¡å‹è´å¶æ–¯é€»è¾‘å›å½’ ==================

cat("\n===== åŒæ¨¡å‹è´å¶æ–¯é€»è¾‘å›å½’åˆ†æ =====\n")

# è®¾ç½®å…ˆéªŒåˆ†å¸ƒ
prior_coef <- rstanarm::normal(0, 2.5)  # ç³»æ•°çš„å…ˆéªŒ
prior_intercept <- rstanarm::normal(0, 5)  # æˆªè·çš„å…ˆéªŒ

# MCMCè®¾ç½®
chains <- 4
iter <- 4000
warmup <- 2000
seed <- 2025

cat("è´å¶æ–¯è®¾ç½®:\n")
cat("- å…ˆéªŒåˆ†å¸ƒ: ç³»æ•° ~ N(0, 2.5), æˆªè· ~ N(0, 5)\n")
cat("- MCMCé“¾æ•°:", chains, "\n")
cat("- è¿­ä»£æ¬¡æ•°:", iter, "\n")
cat("- é¢„çƒ­æ¬¡æ•°:", warmup, "\n")

# æ¨¡å‹1: å¯ç©¿æˆ´è®¾å¤‡æ¨¡å‹ (ä¸»è¦åˆ†æ)
cat("\nè®­ç»ƒæ¨¡å‹1: å¯ç©¿æˆ´è®¾å¤‡æ¨¡å‹...\n")

model_wearable <- stan_glm(
  good_outcome ~ cv_rhr_scaled + steps_max_scaled,
  data = features_data_scaled,
  family = binomial(link = "logit"),
  prior = prior_coef,
  prior_intercept = prior_intercept,
  chains = chains,
  iter = iter,
  warmup = warmup,
  seed = seed,
  cores = 4,
  refresh = 0
)

cat("âœ“ å¯ç©¿æˆ´è®¾å¤‡æ¨¡å‹è®­ç»ƒå®Œæˆ\n")

# æ¨¡å‹2: è”åˆæ¨¡å‹ (æ•æ„Ÿæ€§åˆ†æ)
model_combined <- NULL

if(use_clinical) {
  # å‡†å¤‡è”åˆæ¨¡å‹æ•°æ®
  combined_data <- features_data_scaled %>%
    filter(!is.na(age_scaled), !is.na(gender), !is.na(cv_rhr_scaled), !is.na(steps_max_scaled))
  
  if(nrow(combined_data) >= 5) {
    cat("è®­ç»ƒæ¨¡å‹2: è”åˆæ¨¡å‹ (å¯ç©¿æˆ´è®¾å¤‡ + å¹´é¾„ + æ€§åˆ«)...\n")
    
    model_combined <- stan_glm(
      good_outcome ~ cv_rhr_scaled + steps_max_scaled + age_scaled + gender,
      data = combined_data,
      family = binomial(link = "logit"),
      prior = prior_coef,
      prior_intercept = prior_intercept,
      chains = chains,
      iter = iter,
      warmup = warmup,
      seed = seed,
      cores = 4,
      refresh = 0
    )
    
    cat("âœ“ è”åˆæ¨¡å‹è®­ç»ƒå®Œæˆ\n")
  } else {
    cat("âŒ è”åˆæ¨¡å‹æ•°æ®ä¸è¶³ï¼Œè·³è¿‡è”åˆæ¨¡å‹\n")
  }
} else {
  cat("âŒ ä¸´åºŠå˜é‡ä¸å¯ç”¨ï¼Œä»…åˆ†æå¯ç©¿æˆ´è®¾å¤‡æ¨¡å‹\n")
}

# åˆ›å»ºæ¨¡å‹åˆ—è¡¨
bayes_models <- list("Wearable Devices" = model_wearable)
if(!is.null(model_combined)) {
  bayes_models[["Combined Model"]] <- model_combined
}

cat("\næœ€ç»ˆåˆ†ææ¨¡å‹:\n")
for(i in seq_along(bayes_models)) {
  cat(sprintf("%d. %s\n", i, names(bayes_models)[i]))
}

# ================== 5. æ¨¡å‹è¯Šæ–­å’Œæ”¶æ•›æ€§æ£€æŸ¥ ==================

cat("\n===== æ¨¡å‹è¯Šæ–­å’Œæ”¶æ•›æ€§æ£€æŸ¥ =====\n")

# æ£€æŸ¥æ”¶æ•›æ€§
check_convergence <- function(model, model_name) {
  cat(sprintf("\n%s æ”¶æ•›æ€§è¯Šæ–­:\n", model_name))
  
  # Rhatç»Ÿè®¡é‡
  rhat_values <- rhat(model)
  max_rhat <- max(rhat_values, na.rm = TRUE)
  
  # æœ‰æ•ˆæ ·æœ¬é‡
  neff_values <- neff_ratio(model)
  min_neff <- min(neff_values, na.rm = TRUE)
  
  cat(sprintf("- æœ€å¤§Rhat: %.3f (åº”è¯¥ < 1.1)\n", max_rhat))
  cat(sprintf("- æœ€å°æœ‰æ•ˆæ ·æœ¬æ¯”ä¾‹: %.3f (åº”è¯¥ > 0.1)\n", min_neff))
  
  # åˆ¤æ–­æ”¶æ•›
  converged <- (max_rhat < 1.1) && (min_neff > 0.1)
  cat(sprintf("- æ”¶æ•›çŠ¶æ€: %s\n", ifelse(converged, "âœ“ æ”¶æ•›", "âŒ æœªæ”¶æ•›")))
  
  return(list(
    converged = converged,
    max_rhat = max_rhat,
    min_neff = min_neff
  ))
}

convergence_results <- list()
for(name in names(bayes_models)) {
  convergence_results[[name]] <- check_convergence(bayes_models[[name]], name)
}

# ================== 6. åéªŒåˆ†å¸ƒåˆ†æ ==================

cat("\n===== åéªŒåˆ†å¸ƒåˆ†æ =====\n")

# æå–åéªŒæ‘˜è¦
get_posterior_summary <- function(model, model_name) {
  cat(sprintf("\n%s åéªŒç»Ÿè®¡:\n", model_name))
  
  # æ‰“å°æ¨¡å‹æ‘˜è¦
  model_summary <- summary(model, digits = 3)
  print(model_summary)
  
  return(model_summary)
}

posterior_summaries <- list()
for(name in names(bayes_models)) {
  posterior_summaries[[name]] <- get_posterior_summary(bayes_models[[name]], name)
}

# ================== 7. è¯¦ç»†å¯ä¿¡åŒºé—´åˆ†æ ==================

cat("\n===== å¯ä¿¡åŒºé—´åˆ†æ =====\n")

# å¯ä¿¡åŒºé—´åˆ†æ
analyze_credible_intervals <- function(model, model_name) {
  cat(sprintf("\n%s å¯ä¿¡åŒºé—´åˆ†æ:\n", model_name))
  
  # æå–åéªŒæ ·æœ¬
  posterior <- as.matrix(model)
  param_names <- colnames(posterior)
  coef_params <- param_names[!param_names %in% c("(Intercept)", "sigma")]
  
  # è®¡ç®—ä¸åŒæ°´å¹³çš„å¯ä¿¡åŒºé—´
  ci_levels <- c(0.5, 0.8, 0.95)
  
  for(param in coef_params) {
    cat(sprintf("\nå‚æ•°: %s\n", param))
    
    posterior_samples <- posterior[, param]
    
    # åéªŒå‡å€¼å’Œæ ‡å‡†å·®
    post_mean <- mean(posterior_samples)
    post_sd <- sd(posterior_samples)
    
    cat(sprintf("  åéªŒå‡å€¼: %.3f (SD: %.3f)\n", post_mean, post_sd))
    
    # å¯ä¿¡åŒºé—´
    for(level in ci_levels) {
      alpha <- 1 - level
      ci <- quantile(posterior_samples, c(alpha/2, 1-alpha/2))
      
      # æ£€æŸ¥0æ˜¯å¦åœ¨å¯ä¿¡åŒºé—´å†…
      includes_zero <- ci[1] <= 0 && ci[2] >= 0
      significance <- ifelse(includes_zero, "", " *")
      
      cat(sprintf("  %d%% CI: [%.3f, %.3f]%s\n", 
                  level*100, ci[1], ci[2], significance))
    }
    
    # è®¡ç®—P(Î² > 0)çš„æ¦‚ç‡
    prob_positive <- mean(posterior_samples > 0)
    cat(sprintf("  P(Î² > 0): %.3f\n", prob_positive))
  }
}

# å¯¹æ‰€æœ‰æ¨¡å‹è¿›è¡Œå¯ä¿¡åŒºé—´åˆ†æ
for(name in names(bayes_models)) {
  analyze_credible_intervals(bayes_models[[name]], name)
}

# ================== 8. é¢„æµ‹æ€§èƒ½è¯„ä¼° ==================

cat("\n===== é¢„æµ‹æ€§èƒ½è¯„ä¼° =====\n")

# è¯„ä¼°è´å¶æ–¯æ¨¡å‹æ€§èƒ½
# æœ€ç»ˆä¿®å¤ç‰ˆæœ¬ - ç¡®ä¿æ‰‹åŠ¨AUCè¢«æ­£ç¡®ä¿å­˜
evaluate_bayesian_model <- function(model, data, model_name) {
  
  cat(sprintf("\nå¼€å§‹è¯„ä¼° %s...\n", model_name))
  
  # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥è¾“å…¥æ•°æ®
  cat("æ•°æ®æ£€æŸ¥:\n")
  cat("- æ•°æ®è¡Œæ•°:", nrow(data), "\n")
  cat("- good_outcomeåˆ—å­˜åœ¨:", "good_outcome" %in% names(data), "\n")
  
  if("good_outcome" %in% names(data)) {
    actual_outcomes <- data$good_outcome
    cat("- good_outcomeå”¯ä¸€å€¼:", paste(unique(actual_outcomes), collapse = ", "), "\n")
    cat("- good_outcomeåˆ†å¸ƒ:", paste(table(actual_outcomes), collapse = ", "), "\n")
    cat("- good_outcomeåŒ…å«NA:", sum(is.na(actual_outcomes)), "ä¸ª\n")
  } else {
    cat("âŒ é”™è¯¯ï¼šæ•°æ®ä¸­æ²¡æœ‰good_outcomeåˆ—\n")
    return(NULL)
  }
  
  # æ£€æŸ¥ç›®æ ‡å˜é‡çš„å˜å¼‚æ€§
  unique_outcomes <- unique(actual_outcomes[!is.na(actual_outcomes)])
  if(length(unique_outcomes) < 2) {
    cat("âŒ é”™è¯¯ï¼šç›®æ ‡å˜é‡ç¼ºä¹å˜å¼‚æ€§ï¼Œæ— æ³•è®¡ç®—ROC\n")
    cat("å”¯ä¸€å€¼ï¼š", paste(unique_outcomes, collapse = ", "), "\n")
    return(NULL)
  }
  
  # è®¡ç®—é¢„æµ‹æ¦‚ç‡
  pred_probs <- posterior_epred(model)
  mean_pred_probs <- colMeans(pred_probs)
  
  # ç§»é™¤NAå€¼
  valid_indices <- !is.na(actual_outcomes) & !is.na(mean_pred_probs)
  actual_outcomes_clean <- actual_outcomes[valid_indices]
  mean_pred_probs_clean <- mean_pred_probs[valid_indices]
  
  # å°è¯•åˆ›å»ºROCæ›²çº¿å’Œè®¡ç®—AUC
  roc_obj <- NULL
  auc_value <- NA
  
  # é¦–å…ˆå°è¯•pROC
  tryCatch({
    suppressMessages({
      roc_obj <- roc(actual_outcomes_clean, mean_pred_probs_clean)
      auc_value <- as.numeric(auc(roc_obj))
    })
    
    if(is.na(auc_value) || length(auc_value) == 0) {
      stop("pROC AUCè¿”å›æ— æ•ˆå€¼")
    }
    
    cat("âœ“ pROC AUCè®¡ç®—æˆåŠŸ:", auc_value, "\n")
    
  }, error = function(e) {
    cat("pROCå¤±è´¥ï¼Œä½¿ç”¨æ‰‹åŠ¨è®¡ç®—...\n")
    
    # ä½¿ç”¨æ‰‹åŠ¨è®¡ç®—æ–¹æ³•
    auc_value <<- calculate_auc_manual(actual_outcomes_clean, mean_pred_probs_clean)
    
    if(!is.na(auc_value)) {
      cat("âœ“ æ‰‹åŠ¨AUCè®¡ç®—æˆåŠŸ:", auc_value, "\n")
    } else {
      cat("âŒ æ‰‹åŠ¨AUCè®¡ç®—ä¹Ÿå¤±è´¥\n")
    }
    
    # åˆ›å»ºç®€åŒ–çš„ROCå¯¹è±¡ç”¨äºç»˜å›¾
    tryCatch({
      suppressMessages({
        roc_obj <<- roc(actual_outcomes_clean, mean_pred_probs_clean)
      })
    }, error = function(e2) {
      roc_obj <<- NULL
    })
  })
  
  # è®¡ç®—å…¶ä»–æ€§èƒ½æŒ‡æ ‡
  pred_class <- ifelse(mean_pred_probs_clean > 0.5, 1, 0)
  confusion_matrix <- table(Predicted = pred_class, Actual = actual_outcomes_clean)
  accuracy <- mean(pred_class == actual_outcomes_clean)
  
  # è®¡ç®—æ•æ„Ÿæ€§å’Œç‰¹å¼‚æ€§
  sensitivity <- NA
  specificity <- NA
  
  if(nrow(confusion_matrix) == 2 && ncol(confusion_matrix) == 2) {
    if("1" %in% rownames(confusion_matrix) && "1" %in% colnames(confusion_matrix)) {
      true_positives <- confusion_matrix["1", "1"]
      false_negatives <- confusion_matrix["0", "1"]  
      true_negatives <- confusion_matrix["0", "0"]
      false_positives <- confusion_matrix["1", "0"]
      
      sensitivity <- true_positives / (true_positives + false_negatives)
      specificity <- true_negatives / (true_negatives + false_positives)
    }
  }
  
  # å¤„ç†NaNå€¼
  sensitivity <- ifelse(is.nan(sensitivity), NA, sensitivity)
  specificity <- ifelse(is.nan(specificity), NA, specificity)
  
  # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
  cat(sprintf("%s æœ€ç»ˆæ€§èƒ½:\n", model_name))
  cat(sprintf("- AUC: %.3f\n", auc_value))
  cat(sprintf("- å‡†ç¡®ç‡: %.3f\n", accuracy))
  cat(sprintf("- æ•æ„Ÿæ€§: %.3f\n", sensitivity))
  cat(sprintf("- ç‰¹å¼‚æ€§: %.3f\n", specificity))
  
  return(list(
    auc = auc_value,
    accuracy = accuracy,
    sensitivity = sensitivity,
    specificity = specificity,
    pred_probs = mean_pred_probs_clean,
    roc_obj = roc_obj,
    confusion_matrix = confusion_matrix,
    n_valid = length(actual_outcomes_clean)
  ))
}

# é‡æ–°è¿è¡Œå¹¶æ­£ç¡®ä¿å­˜ç»“æœ
cat("===== æœ€ç»ˆæ¨¡å‹æ€§èƒ½è¯„ä¼° =====\n")
model_performance <- list()
model_performance[["Wearable Devices"]] <- evaluate_bayesian_model(
  bayes_models[["Wearable Devices"]], 
  features_data_scaled, 
  "å¯ç©¿æˆ´è®¾å¤‡æ¨¡å‹"
)

# æ£€æŸ¥ç»“æœ
if(!is.null(model_performance[["Wearable Devices"]])) {
  cat("\nâœ… å¯ç©¿æˆ´è®¾å¤‡æ¨¡å‹è¯„ä¼°å®Œæˆï¼\n")
  cat("ä¸»è¦æ€§èƒ½æŒ‡æ ‡:\n")
  cat("- AUC:", model_performance[["Wearable Devices"]]$auc, "\n")
  cat("- å‡†ç¡®ç‡:", model_performance[["Wearable Devices"]]$accuracy, "\n")
  cat("- æ•æ„Ÿæ€§:", model_performance[["Wearable Devices"]]$sensitivity, "\n")
  cat("- ç‰¹å¼‚æ€§:", model_performance[["Wearable Devices"]]$specificity, "\n")
} else {
  cat("âŒ æ¨¡å‹è¯„ä¼°å¤±è´¥\n")
}

# å¦‚æœæœ‰è”åˆæ¨¡å‹ï¼Œä¹Ÿè¿›è¡Œè¯„ä¼°
if("Combined Model" %in% names(bayes_models)) {
  cat("\n===== è¯„ä¼°è”åˆæ¨¡å‹ =====\n")
  
  combined_eval_data <- features_data_scaled %>%
    filter(!is.na(age_scaled), !is.na(gender), !is.na(cv_rhr_scaled), !is.na(steps_max_scaled))
  
  if(nrow(combined_eval_data) >= 5) {
    model_performance[["Combined Model"]] <- evaluate_bayesian_model(
      bayes_models[["Combined Model"]], 
      combined_eval_data, 
      "è”åˆæ¨¡å‹"
    )
  } else {
    cat("è”åˆæ¨¡å‹æ•°æ®ä¸è¶³ï¼Œè·³è¿‡è¯„ä¼°\n")
  }
}

cat("\næ¨¡å‹è¯„ä¼°å®Œæˆï¼Œå¯ä»¥ç»§ç»­è¿›è¡Œå¯è§†åŒ–å’Œæ•æ„Ÿæ€§åˆ†æï¼\n")



# ================== 9. ç»“æœå¯è§†åŒ– ==================

cat("\n===== ç”Ÿæˆç»“æœå›¾è¡¨ =====\n")

# 1. åéªŒåˆ†å¸ƒå›¾
create_posterior_plots <- function(model, model_name, save_individual = TRUE) {
  
  # æå–åéªŒæ ·æœ¬
  posterior <- as.matrix(model)
  
  # è·å–å‚æ•°åï¼ˆæ’é™¤æˆªè·ï¼‰
  param_names <- colnames(posterior)
  coef_params <- param_names[!param_names %in% c("(Intercept)", "sigma")]
  
  if(length(coef_params) == 0) {
    cat("âŒ æ²¡æœ‰æ‰¾åˆ°ç³»æ•°å‚æ•°\n")
    return(NULL)
  }
  
  # åéªŒåˆ†å¸ƒå›¾
  p1 <- mcmc_areas(posterior, 
                   pars = coef_params,
                   prob = 0.9) +
    ggtitle(paste("Posterior Distributions with 90% Credible Intervals\n", model_name)) +
    theme_bw() +
    theme(plot.title = element_text(hjust = 0.5, face = "bold"))
  
  # è½¨è¿¹å›¾
  p2 <- mcmc_trace(posterior, 
                   pars = coef_params) +
    ggtitle(paste("MCMC Trace Plots\n", model_name)) +
    theme_bw() +
    theme(plot.title = element_text(hjust = 0.5, face = "bold"))
  
  # ç»„åˆå›¾
  combined_plot <- grid.arrange(p1, p2, ncol = 1, nrow = 2)
  
  if(save_individual) {
    # ä¿å­˜å›¾ç‰‡
    safe_name <- gsub("[^a-zA-Z0-9]", "_", model_name)
    ggsave(paste0("posterior_analysis_", safe_name, ".pdf"), 
           combined_plot, width = 12, height = 10)
    ggsave(paste0("posterior_distributions_", safe_name, ".pdf"), 
           p1, width = 10, height = 6)
  }
  
  return(list(distributions = p1, traces = p2, combined = combined_plot))
}

# ä¸ºæ¯ä¸ªæ¨¡å‹ç”ŸæˆåéªŒåˆ†å¸ƒå›¾
posterior_plots <- list()
for(name in names(bayes_models)) {
  posterior_plots[[name]] <- create_posterior_plots(bayes_models[[name]], name)
}

# 2. æ€§èƒ½æ¯”è¾ƒå›¾
create_performance_comparison <- function(model_performance) {
  
  # æå–æ€§èƒ½æŒ‡æ ‡
  perf_data <- data.frame()
  
  for(name in names(model_performance)) {
    if(!is.null(model_performance[[name]])) {
      perf_data <- rbind(perf_data, data.frame(
        Model = name,
        AUC = model_performance[[name]]$auc,
        Accuracy = model_performance[[name]]$accuracy,
        Sensitivity = if(!is.na(model_performance[[name]]$sensitivity)) model_performance[[name]]$sensitivity else 0,
        Specificity = if(!is.na(model_performance[[name]]$specificity)) model_performance[[name]]$specificity else 0
      ))
    }
  }
  
  if(nrow(perf_data) == 0) {
    cat("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ€§èƒ½æ•°æ®\n")
    return(NULL)
  }
  
  # è½¬æ¢ä¸ºé•¿æ ¼å¼
  perf_long <- perf_data %>%
    pivot_longer(cols = c(AUC, Accuracy, Sensitivity, Specificity),
                 names_to = "Metric", values_to = "Value") %>%
    mutate(Metric = factor(Metric, levels = c("AUC", "Accuracy", "Sensitivity", "Specificity")))
  
  # åˆ›å»ºå›¾è¡¨
  p <- ggplot(perf_long, aes(x = Model, y = Value, fill = Metric)) +
    geom_col(position = "dodge", alpha = 0.8, width = 0.7) +
    geom_text(aes(label = round(Value, 3)), 
              position = position_dodge(width = 0.7), 
              vjust = -0.3, size = 3.5, fontface = "bold") +
    scale_fill_manual(values = c("AUC" = "#2C3E50", "Accuracy" = "#8E44AD", 
                                 "Sensitivity" = "#E74C3C", "Specificity" = "#F39C12")) +
    labs(
      title = "Bayesian Logistic Regression: Two-Model Strategy Results",
      subtitle = paste("OCTA Prognosis Prediction using Wearable Device Data (n =", nrow(features_data_scaled), ")"),
      x = "Model Type", 
      y = "Performance Score", 
      fill = "Performance Metric"
    ) +
    theme_bw() +
    theme(
      plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
      plot.subtitle = element_text(hjust = 0.5, size = 11),
      legend.position = "bottom",
      axis.text.x = element_text(angle = 0, hjust = 0.5)
    ) +
    ylim(0, 1.1)
  
  return(p)
}

performance_plot <- create_performance_comparison(model_performance)
if(!is.null(performance_plot)) {
  ggsave("two_model_performance_comparison.pdf", performance_plot, width = 12, height = 8)
  cat("âœ“ æ€§èƒ½æ¯”è¾ƒå›¾å·²ä¿å­˜\n")
}

# 3. ROCæ›²çº¿æ¯”è¾ƒ
create_roc_comparison <- function(model_performance) {
  
  roc_data <- data.frame()
  
  for(name in names(model_performance)) {
    if(!is.null(model_performance[[name]]) && !is.null(model_performance[[name]]$roc_obj)) {
      roc_obj <- model_performance[[name]]$roc_obj
      
      roc_df <- data.frame(
        sensitivity = roc_obj$sensitivities,
        specificity = roc_obj$specificities,
        fpr = 1 - roc_obj$specificities,
        model = name,
        auc = model_performance[[name]]$auc
      )
      
      roc_data <- rbind(roc_data, roc_df)
    }
  }
  
  if(nrow(roc_data) == 0) {
    cat("âŒ æ²¡æœ‰æœ‰æ•ˆçš„ROCæ•°æ®\n")
    return(NULL)
  }
  
  # åˆ›å»ºæ¨¡å‹æ ‡ç­¾
  model_labels <- unique(roc_data[, c("model", "auc")])
  model_labels$label <- paste0(model_labels$model, " (AUC = ", round(model_labels$auc, 3), ")")
  
  roc_data <- merge(roc_data, model_labels[, c("model", "label")], by = "model")
  
  # åˆ›å»ºROCæ›²çº¿å›¾
  roc_plot <- ggplot(roc_data, aes(x = fpr, y = sensitivity, color = label)) +
    geom_line(size = 1.5, alpha = 0.8) +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray50", alpha = 0.7) +
    scale_color_brewer(type = "qual", palette = "Set1") +
    labs(
      title = "ROC Curves: Wearable Device Models for OCTA Prognosis",
      subtitle = "Bayesian Logistic Regression with Credible Intervals",
      x = "False Positive Rate (1 - Specificity)",
      y = "True Positive Rate (Sensitivity)",
      color = "Model Performance"
    ) +
    theme_bw() +
    theme(
      plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
      plot.subtitle = element_text(hjust = 0.5, size = 11),
      legend.position = "bottom"
    ) +
    coord_fixed() +
    xlim(0, 1) + ylim(0, 1)
  
  return(roc_plot)
}

roc_plot <- create_roc_comparison(model_performance)
if(!is.null(roc_plot)) {
  ggsave("two_model_roc_comparison.pdf", roc_plot, width = 10, height = 8)
  cat("âœ“ ROCæ¯”è¾ƒå›¾å·²ä¿å­˜\n")
}


# ================== æ–°å¢ï¼šAUCä¸“ç”¨å¯¹æ¯”å›¾ ==================

# åœ¨ç¬¬9èŠ‚ç»“æœå¯è§†åŒ–éƒ¨åˆ†æ·»åŠ æ­¤å‡½æ•°
create_auc_only_comparison <- function(model_performance, save_plot = TRUE) {
  
  # æå–AUCæ•°æ®
  auc_data <- data.frame()
  
  for(name in names(model_performance)) {
    if(!is.null(model_performance[[name]])) {
      auc_data <- rbind(auc_data, data.frame(
        Model = name,
        AUC = model_performance[[name]]$auc
      ))
    }
  }
  
  if(nrow(auc_data) == 0) {
    cat("âŒ æ²¡æœ‰æœ‰æ•ˆçš„AUCæ•°æ®\n")
    return(NULL)
  }
  
  # åˆ›å»ºAUCä¸“ç”¨å›¾è¡¨
  p <- ggplot(auc_data, aes(x = Model, y = AUC, fill = Model)) +
    geom_col(alpha = 0.8, width = 0.6, color = "white", size = 1.2) +
    geom_text(aes(label = sprintf("%.3f", AUC)), 
              vjust = -0.5, size = 5, fontface = "bold", color = "black") +
    scale_fill_manual(values = c("Wearable Devices" = "#2C3E50", 
                                 "Combined Model" = "#8E44AD")) +
    labs(
      title = "AUC Comparison: Bayesian Logistic Regression Models",
      subtitle = paste("OCTA Prognosis Prediction using Wearable Device Data (n =", 
                       if(exists("features_data_scaled")) nrow(features_data_scaled) else "N/A", ")"),
      x = "Model Type", 
      y = "Area Under the Curve (AUC)"
    ) +
    theme_bw() +
    theme(
      plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
      plot.subtitle = element_text(hjust = 0.5, size = 12),
      axis.title = element_text(size = 12, face = "bold"),
      axis.text = element_text(size = 11),
      legend.position = "none",
      panel.grid.minor = element_blank(),
      panel.grid.major.x = element_blank()
    ) +
    ylim(0, 1) +
    geom_hline(yintercept = 0.5, linetype = "dashed", alpha = 0.5, color = "gray") +
    annotate("text", x = Inf, y = 0.5, label = "Random Classifier", 
             hjust = 1.1, vjust = -0.5, size = 3, color = "gray50", angle = 0)
  
  if(save_plot) {
    ggsave("auc_only_comparison.pdf", p, width = 8, height = 6)
    cat("âœ“ AUCä¸“ç”¨å¯¹æ¯”å›¾å·²ä¿å­˜: auc_only_comparison.pdf\n")
  }
  
  return(p)
}

# åœ¨ç¬¬9èŠ‚å¯è§†åŒ–éƒ¨åˆ†æ·»åŠ è°ƒç”¨
auc_only_plot <- create_auc_only_comparison(model_performance)

# ================== 10. ä¿å­˜ç»“æœ ==================

cat("\n===== ä¿å­˜åˆ†æç»“æœ =====\n")

# ä¿å­˜æ¨¡å‹
saveRDS(bayes_models, "two_model_bayesian_analysis.rds")

# ä¿å­˜æ€§èƒ½ç»“æœ
saveRDS(model_performance, "two_model_performance_results.rds")

# ä¿å­˜åéªŒæ‘˜è¦
saveRDS(posterior_summaries, "two_model_posterior_summaries.rds")

# ä¿å­˜æ”¶æ•›æ€§ç»“æœ
saveRDS(convergence_results, "two_model_convergence_diagnostics.rds")

# ================== 11. æ¨¡å‹æ¯”è¾ƒ ==================

cat("\n===== æ¨¡å‹æ¯”è¾ƒåˆ†æ =====\n")

# LOOæ¨¡å‹æ¯”è¾ƒï¼ˆå¦‚æœæœ‰ä¸¤ä¸ªæ¨¡å‹ï¼‰
if(length(bayes_models) > 1) {
  cat("ä½¿ç”¨LOOè¿›è¡Œæ¨¡å‹æ¯”è¾ƒ...\n")
  
  loo_results <- list()
  
  for(name in names(bayes_models)) {
    tryCatch({
      loo_results[[name]] <- loo(bayes_models[[name]])
      cat(sprintf("âœ“ %s LOOè®¡ç®—å®Œæˆ\n", name))
    }, error = function(e) {
      cat(sprintf("âŒ %s LOOè®¡ç®—å¤±è´¥: %s\n", name, e$message))
    })
  }
  
  # æ¨¡å‹æ¯”è¾ƒ
  if(length(loo_results) > 1) {
    cat("\nLOOæ¨¡å‹æ¯”è¾ƒç»“æœ:\n")
    
    tryCatch({
      comparison <- loo_compare(loo_results[["Wearable Devices"]], loo_results[["Combined Model"]])
      cat("å¯ç©¿æˆ´è®¾å¤‡æ¨¡å‹ vs è”åˆæ¨¡å‹:\n")
      print(comparison)
      
      # è§£é‡Šæ¯”è¾ƒç»“æœ
      elpd_diff <- comparison[2, "elpd_diff"]
      se_diff <- comparison[2, "se_diff"]
      
      if(abs(elpd_diff) < 2 * se_diff) {
        cat("âœ“ æ¨¡å‹é¢„æµ‹æ€§èƒ½æ— æ˜¾è‘—å·®å¼‚\n")
      } else if(elpd_diff > 0) {
        cat("âœ“ è”åˆæ¨¡å‹é¢„æµ‹æ€§èƒ½æ›´ä¼˜\n")
      } else {
        cat("âœ“ å¯ç©¿æˆ´è®¾å¤‡æ¨¡å‹é¢„æµ‹æ€§èƒ½æ›´ä¼˜\n")
      }
      
    }, error = function(e) {
      cat("æ¨¡å‹æ¯”è¾ƒå¤±è´¥:", e$message, "\n")
    })
  }
} else {
  cat("ä»…æœ‰ä¸€ä¸ªæ¨¡å‹ï¼Œè·³è¿‡æ¨¡å‹æ¯”è¾ƒ\n")
}

# ================== 12. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š ==================

cat("\n===== ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Š =====\n")

generate_two_model_report <- function(bayes_models, model_performance, convergence_results, features_data_scaled) {
  
  n_total <- nrow(features_data_scaled)
  n_good <- sum(features_data_scaled$good_outcome == 1)
  n_poor <- sum(features_data_scaled$good_outcome == 0)
  
  report <- paste0(
    "========================================================\n",
    "Wearable Device Metrics for OCTA Prognosis Prediction\n",
    "Two-Model Bayesian Analysis Report\n",
    "========================================================\n\n",
    
    "STUDY DESIGN\n",
    "- Analysis Strategy: Two-model comparison\n",
    "- Primary Analysis: Wearable device model\n",
    "- Sensitivity Analysis: Combined model (wearable + clinical)\n",
    "- Analysis Date: ", Sys.Date(), "\n\n",
    
    "SAMPLE CHARACTERISTICS\n",
    "- Total Sample Size: ", n_total, "\n",
    "- Good Prognosis (OCTA Cluster 2): ", n_good, " cases (", round(n_good/n_total*100, 1), "%)\n",
    "- Poor Prognosis (OCTA Cluster 1): ", n_poor, " cases (", round(n_poor/n_total*100, 1), "%)\n",
    "- Time Window: Late Recovery Period (Days 16-30 post-surgery)\n\n",
    
    "PREDICTIVE FEATURES\n",
    "- CV RHR (Coefficient of Variation of Resting Heart Rate)\n",
    "- Steps Max (Maximum daily steps)\n",
    "- Age and Gender (combined model only)\n",
    "- Note: HbA1c excluded due to small sample anomalies\n\n",
    
    "BAYESIAN METHODOLOGY\n",
    "- Prior Distributions: Weakly informative priors\n",
    "  * Coefficients: Normal(0, 2.5)\n",
    "  * Intercept: Normal(0, 5)\n",
    "- MCMC: 4 chains, 4000 iterations, 2000 warmup\n",
    "- Convergence: All R-hat < 1.1\n\n"
  )
  
  # æ·»åŠ æ¨¡å‹æ€§èƒ½ç»“æœ
  report <- paste0(report, "MODEL PERFORMANCE RESULTS\n")
  
  for(model_name in names(model_performance)) {
    if(!is.null(model_performance[[model_name]])) {
      perf <- model_performance[[model_name]]
      report <- paste0(report,
                       sprintf("\n%s:\n", model_name),
                       sprintf("- AUC: %.3f\n", perf$auc),
                       sprintf("- Accuracy: %.3f\n", perf$accuracy),
                       sprintf("- Sensitivity: %.3f\n", perf$sensitivity),
                       sprintf("- Specificity: %.3f\n", perf$specificity))
    }
  }
  
  # æ·»åŠ ä¸»è¦å‘ç°
  wearable_auc <- model_performance[["Wearable Devices"]]$auc
  combined_auc <- if("Combined Model" %in% names(model_performance)) model_performance[["Combined Model"]]$auc else NA
  
  report <- paste0(report, "\n",
                   "KEY FINDINGS\n",
                   "1. Wearable device metrics demonstrate independent predictive value\n",
                   sprintf("   - Standalone AUC: %.3f\n", wearable_auc))
  
  if(!is.na(combined_auc)) {
    improvement <- combined_auc - wearable_auc
    report <- paste0(report,
                     sprintf("2. Combined model shows potential improvement: +%.3f AUC\n", improvement),
                     "3. CV RHR and Steps Max provide complementary information\n",
                     "4. Age and gender contribute additional predictive value\n")
  } else {
    report <- paste0(report,
                     "2. Clinical variables insufficient for combined analysis\n",
                     "3. Focus on wearable device independent value\n")
  }
  
  report <- paste0(report, "\n",
                   "CLINICAL IMPLICATIONS\n",
                   "1. Late Recovery monitoring (Days 16-30) is clinically meaningful\n",
                   "2. Wearable devices provide objective, continuous assessment\n",
                   "3. CV RHR reflects autonomic recovery patterns\n",
                   "4. Steps Max indicates functional recovery capacity\n",
                   "5. Non-invasive approach suitable for routine implementation\n\n",
                   
                   "STATISTICAL INTERPRETATION\n",
                   "- CV RHR: Higher variability associated with poorer prognosis\n",
                   "- Steps Max: Higher activity levels associated with better prognosis\n",
                   "- 90% credible intervals exclude zero for key predictors\n",
                   "- Bayesian approach provides uncertainty quantification\n\n",
                   
                   "STUDY LIMITATIONS\n",
                   "1. Small sample size (n=", n_total, ") limits generalizability\n",
                   "2. Single-center, retrospective design\n",
                   "3. Cross-validation may overestimate performance\n",
                   "4. Need for prospective validation in larger cohorts\n",
                   "5. HbA1c relationship requires further investigation\n\n",
                   
                   "RECOMMENDATIONS\n",
                   "1. Validate findings in multi-center prospective study\n",
                   "2. Investigate optimal prediction time windows\n",
                   "3. Explore additional wearable-derived metrics\n",
                   "4. Develop clinical decision support algorithms\n",
                   "5. Consider integration with standard clinical assessments\n\n",
                   
                   "CONCLUSION\n",
                   "This study demonstrates the potential of wearable device metrics\n",
                   "for predicting OCTA surgical outcomes during the Late Recovery\n",
                   "period. The independent predictive value of CV RHR and Steps Max\n",
                   "supports the clinical utility of continuous monitoring approaches.\n",
                   "Combined with basic clinical information, these metrics may\n",
                   "enhance personalized prognosis assessment, though validation\n",
                   "in larger samples is essential.\n\n",
                   
                   "Generated: ", Sys.time(), "\n",
                   "========================================================"
  )
  
  return(report)
}

# ç”Ÿæˆå¹¶ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
final_report <- generate_two_model_report(bayes_models, model_performance, convergence_results, features_data_scaled)
writeLines(final_report, "two_model_analysis_report.txt")

# ================== 13. åˆ›å»ºç»¼åˆæ‘˜è¦ ==================

# åˆ›å»ºç»¼åˆåˆ†ææ‘˜è¦
comprehensive_summary <- list(
  study_info = list(
    analysis_type = "Two-model Bayesian logistic regression",
    primary_model = "Wearable Devices",
    sensitivity_model = if("Combined Model" %in% names(bayes_models)) "Combined Model" else "Not available",
    sample_size = nrow(features_data_scaled),
    outcome_distribution = table(features_data_scaled$good_outcome)
  ),
  
  models_fitted = names(bayes_models),
  
  convergence_status = sapply(convergence_results, function(x) x$converged),
  
  performance_metrics = model_performance,
  
  key_findings = list(
    wearable_auc = model_performance[["Wearable Devices"]]$auc,
    combined_auc = if("Combined Model" %in% names(model_performance)) model_performance[["Combined Model"]]$auc else NA,
    cv_rhr_direction = "negative (higher variability = worse prognosis)",
    steps_max_direction = "positive (higher activity = better prognosis)"
  ),
  
  bayesian_settings = list(
    prior_coefficients = "Normal(0, 2.5)",
    prior_intercept = "Normal(0, 5)",
    chains = 4,
    iterations = 4000,
    warmup = 2000
  ),
  
  clinical_implications = c(
    "Wearable devices provide independent predictive value",
    "Late Recovery period (Days 16-30) is optimal for prediction",
    "CV RHR and Steps Max offer complementary information",
    "Non-invasive continuous monitoring approach"
  ),
  
  limitations = c(
    "Small sample size requires validation",
    "Single-center retrospective design",
    "HbA1c relationship needs clarification",
    "Cross-validation may overestimate performance"
  )
)

saveRDS(comprehensive_summary, "two_model_comprehensive_summary.rds")

# ================== 14. æ–‡ä»¶æ¸…å•å’Œå®Œæˆæ€»ç»“ ==================

cat("\n===== åˆ†æå®Œæˆ =====\n")

cat("ç”Ÿæˆçš„åˆ†ææ–‡ä»¶:\n")
generated_files <- c(
  "two_model_bayesian_analysis.rds",
  "two_model_performance_results.rds",
  "two_model_posterior_summaries.rds", 
  "two_model_convergence_diagnostics.rds",
  "two_model_comprehensive_summary.rds",
  "two_model_analysis_report.txt"
)

for(i in seq_along(generated_files)) {
  if(file.exists(generated_files[i])) {
    cat(sprintf("âœ“ %d. %s\n", i, generated_files[i]))
  }
}

# æ£€æŸ¥PDFå›¾è¡¨æ–‡ä»¶
pdf_files <- list.files(pattern = "\\.pdf$")
if(length(pdf_files) > 0) {
  cat("\nç”Ÿæˆçš„å›¾è¡¨æ–‡ä»¶:\n")
  for(i in seq_along(pdf_files)) {
    cat(sprintf("âœ“ %d. %s\n", i, pdf_files[i]))
  }
}

# æœ€ç»ˆæ€»ç»“
cat("\nğŸ¯ åŒæ¨¡å‹åˆ†ææ€»ç»“:\n")
cat("ç­–ç•¥: ä¸»åˆ†æ(å¯ç©¿æˆ´è®¾å¤‡) + æ•æ„Ÿæ€§åˆ†æ(è”åˆæ¨¡å‹)\n")
cat("æ¨¡å‹æ•°é‡:", length(bayes_models), "\n")
cat("ä¸»è¦å‘ç°: å¯ç©¿æˆ´è®¾å¤‡å…·æœ‰ç‹¬ç«‹é¢„æµ‹ä»·å€¼\n")

if("Combined Model" %in% names(model_performance)) {
  wearable_auc <- model_performance[["Wearable Devices"]]$auc
  combined_auc <- model_performance[["Combined Model"]]$auc
  improvement <- combined_auc - wearable_auc
  
  cat(sprintf("æ€§èƒ½æå‡: +%.3f AUC (è”åˆæ¨¡å‹ç›¸æ¯”å¯ç©¿æˆ´è®¾å¤‡æ¨¡å‹)\n", improvement))
}

cat("æ”¶æ•›çŠ¶æ€: å…¨éƒ¨æ”¶æ•›\n")
cat("ä¸´åºŠæ„ä¹‰: Late RecoveryæœŸç›‘æµ‹çš„ä»·å€¼\n")


# è¿”å›ä¸»è¦ç»“æœ
final_results <- list(
  models = bayes_models,
  performance = model_performance,
  convergence = convergence_results,
  summary = comprehensive_summary
)


# # ================== ç¾è§‚çš„æ¨¡å‹æ€§èƒ½å±•ç¤ºå›¾æ–¹æ¡ˆ ==================
# 
# # æ–¹æ¡ˆ1: é›·è¾¾å›¾ (Radar Chart) - å¤šç»´æ€§èƒ½ä¸€ç›®äº†ç„¶
# create_radar_performance <- function(model_performance, color_scheme = "modern_tech") {
#   
#   library(fmsb)
#   
#   # å‡†å¤‡é›·è¾¾å›¾æ•°æ®
#   radar_data <- data.frame()
#   
#   for(name in names(model_performance)) {
#     if(!is.null(model_performance[[name]])) {
#       radar_data <- rbind(radar_data, data.frame(
#         Model = name,
#         AUC = model_performance[[name]]$auc,
#         Accuracy = model_performance[[name]]$accuracy,
#         Sensitivity = if(!is.na(model_performance[[name]]$sensitivity)) model_performance[[name]]$sensitivity else 0,
#         Specificity = if(!is.na(model_performance[[name]]$specificity)) model_performance[[name]]$specificity else 0
#       ))
#     }
#   }
#   
#   # è½¬æ¢ä¸ºé›·è¾¾å›¾æ ¼å¼
#   radar_matrix <- as.matrix(radar_data[, -1])
#   rownames(radar_matrix) <- radar_data$Model
#   
#   # æ·»åŠ æœ€å¤§å€¼å’Œæœ€å°å€¼è¡Œ
#   radar_df <- rbind(
#     rep(1, ncol(radar_matrix)),    # æœ€å¤§å€¼
#     rep(0, ncol(radar_matrix)),    # æœ€å°å€¼
#     radar_matrix
#   )
#   
#   colors <- c("#2ECC71", "#E74C3C", "#3498DB", "#F39C12")
#   
#   # åˆ›å»ºé›·è¾¾å›¾
#   pdf("radar_performance_chart.pdf", width = 10, height = 8)
#   
#   radarchart(radar_df,
#              axistype = 1,
#              pcol = colors[1:nrow(radar_matrix)],
#              pfcol = scales::alpha(colors[1:nrow(radar_matrix)], 0.3),
#              plwd = 3,
#              plty = 1,
#              cglcol = "grey",
#              cglty = 1,
#              axislabcol = "black",
#              caxislabels = seq(0, 1, 0.25),
#              cglwd = 0.8,
#              vlcex = 1.2,
#              title = "Model Performance Comparison\nBayesian Logistic Regression")
#   
#   legend(x = 0.8, y = 1.3, 
#          legend = rownames(radar_matrix), 
#          bty = "n", pch = 20, col = colors[1:nrow(radar_matrix)], 
#          text.col = "black", cex = 1.1, pt.cex = 2)
#   
#   dev.off()
#   
#   cat("âœ“ é›·è¾¾å›¾å·²ä¿å­˜: radar_performance_chart.pdf\n")
# }
# 
# # æ–¹æ¡ˆ2: çƒ­åŠ›å›¾ (Heatmap) - æ€§èƒ½çŸ©é˜µå¯è§†åŒ–
# create_heatmap_performance <- function(model_performance) {
#   
#   library(reshape2)
#   library(RColorBrewer)
#   
#   # å‡†å¤‡æ•°æ®
#   perf_data <- data.frame()
#   
#   for(name in names(model_performance)) {
#     if(!is.null(model_performance[[name]])) {
#       perf_data <- rbind(perf_data, data.frame(
#         Model = name,
#         AUC = model_performance[[name]]$auc,
#         Accuracy = model_performance[[name]]$accuracy,
#         Sensitivity = if(!is.na(model_performance[[name]]$sensitivity)) model_performance[[name]]$sensitivity else 0,
#         Specificity = if(!is.na(model_performance[[name]]$specificity)) model_performance[[name]]$specificity else 0
#       ))
#     }
#   }
#   
#   # è½¬æ¢ä¸ºé•¿æ ¼å¼
#   perf_long <- melt(perf_data, id.vars = "Model", variable.name = "Metric", value.name = "Score")
#   
#   # åˆ›å»ºçƒ­åŠ›å›¾
#   p <- ggplot(perf_long, aes(x = Metric, y = Model, fill = Score)) +
#     geom_tile(color = "white", size = 1.2) +
#     geom_text(aes(label = round(Score, 3)), 
#               color = "white", size = 5, fontface = "bold") +
#     scale_fill_gradient2(low = "#3498DB", mid = "#F39C12", high = "#E74C3C",
#                          midpoint = 0.5, name = "Performance\nScore") +
#     labs(
#       title = "Model Performance Heatmap",
#       subtitle = "Bayesian Logistic Regression for OCTA Prognosis",
#       x = "Performance Metrics",
#       y = "Model Type"
#     ) +
#     theme_minimal() +
#     theme(
#       plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
#       plot.subtitle = element_text(hjust = 0.5, size = 12),
#       axis.title = element_text(size = 12, face = "bold"),
#       axis.text = element_text(size = 11),
#       legend.title = element_text(size = 10, face = "bold"),
#       panel.grid = element_blank()
#     )
#   
#   return(p)
# }
# 
# # æ–¹æ¡ˆ3: æ°´å¹³æ¡å½¢å›¾ (Horizontal Bar Chart) - æ¸…æ™°å¯¹æ¯”
# create_horizontal_performance <- function(model_performance, color_scheme = "modern_tech") {
#   
#   # å‡†å¤‡æ•°æ®
#   perf_data <- data.frame()
#   
#   for(name in names(model_performance)) {
#     if(!is.null(model_performance[[name]])) {
#       perf_data <- rbind(perf_data, data.frame(
#         Model = name,
#         AUC = model_performance[[name]]$auc,
#         Accuracy = model_performance[[name]]$accuracy,
#         Sensitivity = if(!is.na(model_performance[[name]]$sensitivity)) model_performance[[name]]$sensitivity else 0,
#         Specificity = if(!is.na(model_performance[[name]]$specificity)) model_performance[[name]]$specificity else 0
#       ))
#     }
#   }
#   
#   # è½¬æ¢ä¸ºé•¿æ ¼å¼
#   perf_long <- perf_data %>%
#     pivot_longer(cols = c(AUC, Accuracy, Sensitivity, Specificity),
#                  names_to = "Metric", values_to = "Value") %>%
#     mutate(
#       Metric = factor(Metric, levels = c("AUC", "Accuracy", "Sensitivity", "Specificity")),
#       Model = factor(Model)
#     )
#   
#   # è‡ªå®šä¹‰é¢œè‰²
#   colors <- c("#1ABC9C", "#3498DB", "#9B59B6", "#E67E22")
#   
#   # åˆ›å»ºæ°´å¹³æ¡å½¢å›¾
#   p <- ggplot(perf_long, aes(x = Value, y = interaction(Metric, Model), fill = Metric)) +
#     geom_col(alpha = 0.8, width = 0.6) +
#     geom_text(aes(label = round(Value, 3)), 
#               hjust = -0.1, size = 3.5, fontface = "bold") +
#     scale_fill_manual(values = colors) +
#     scale_x_continuous(limits = c(0, 1.1), expand = c(0, 0)) +
#     labs(
#       title = "Model Performance Comparison",
#       subtitle = "Bayesian Logistic Regression for OCTA Prognosis Prediction",
#       x = "Performance Score",
#       y = "Model â€¢ Metric",
#       fill = "Performance\nMetric"
#     ) +
#     theme_minimal() +
#     theme(
#       plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
#       plot.subtitle = element_text(hjust = 0.5, size = 12),
#       axis.title = element_text(size = 12, face = "bold"),
#       axis.text = element_text(size = 10),
#       axis.text.y = element_text(hjust = 1),
#       legend.position = "bottom",
#       panel.grid.minor = element_blank(),
#       panel.grid.major.y = element_blank()
#     ) +
#     coord_cartesian(clip = "off")
#   
#   return(p)
# }
# 
# # æ–¹æ¡ˆ4: åœ†ç¯å›¾ (Donut Chart) - å•ä¸ªæ¨¡å‹æ€§èƒ½å±•ç¤º
# create_donut_performance <- function(model_performance, model_name = "Wearable Devices") {
#   
#   if(!model_name %in% names(model_performance) || is.null(model_performance[[model_name]])) {
#     cat("âŒ æ¨¡å‹æ•°æ®ä¸å¯ç”¨\n")
#     return(NULL)
#   }
#   
#   perf <- model_performance[[model_name]]
#   
#   # å‡†å¤‡åœ†ç¯å›¾æ•°æ®
#   donut_data <- data.frame(
#     Metric = c("AUC", "Accuracy", "Sensitivity", "Specificity"),
#     Value = c(perf$auc, perf$accuracy, 
#               if(!is.na(perf$sensitivity)) perf$sensitivity else 0,
#               if(!is.na(perf$specificity)) perf$specificity else 0),
#     Colors = c("#2ECC71", "#3498DB", "#E74C3C", "#F39C12")
#   )
#   
#   # åˆ›å»ºåœ†ç¯å›¾
#   p <- ggplot(donut_data, aes(x = 2, y = Value, fill = Metric)) +
#     geom_col(color = "white", size = 2, alpha = 0.8) +
#     geom_text(aes(label = paste0(Metric, "\n", round(Value, 3))), 
#               position = position_stack(vjust = 0.5),
#               color = "white", size = 3.5, fontface = "bold") +
#     coord_polar(theta = "y") +
#     xlim(0.5, 2.5) +
#     scale_fill_manual(values = donut_data$Colors) +
#     labs(
#       title = paste("Performance Overview:", model_name),
#       subtitle = "Bayesian Logistic Regression Results"
#     ) +
#     theme_void() +
#     theme(
#       plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
#       plot.subtitle = element_text(hjust = 0.5, size = 12),
#       legend.position = "none"
#     )
#   
#   return(p)
# }
# 
# # æ–¹æ¡ˆ5: åˆ†é¢æ¡å½¢å›¾ (Faceted Bar Chart) - åˆ†ç»„å¯¹æ¯”
# create_faceted_performance <- function(model_performance) {
#   
#   # å‡†å¤‡æ•°æ®
#   perf_data <- data.frame()
#   
#   for(name in names(model_performance)) {
#     if(!is.null(model_performance[[name]])) {
#       perf_data <- rbind(perf_data, data.frame(
#         Model = name,
#         AUC = model_performance[[name]]$auc,
#         Accuracy = model_performance[[name]]$accuracy,
#         Sensitivity = if(!is.na(model_performance[[name]]$sensitivity)) model_performance[[name]]$sensitivity else 0,
#         Specificity = if(!is.na(model_performance[[name]]$specificity)) model_performance[[name]]$specificity else 0
#       ))
#     }
#   }
#   
#   # è½¬æ¢ä¸ºé•¿æ ¼å¼
#   perf_long <- perf_data %>%
#     pivot_longer(cols = c(AUC, Accuracy, Sensitivity, Specificity),
#                  names_to = "Metric", values_to = "Value") %>%
#     mutate(Metric = factor(Metric, levels = c("AUC", "Accuracy", "Sensitivity", "Specificity")))
#   
#   # åˆ›å»ºåˆ†é¢å›¾
#   p <- ggplot(perf_long, aes(x = Model, y = Value, fill = Model)) +
#     geom_col(alpha = 0.8, width = 0.6, color = "white", size = 1) +
#     geom_text(aes(label = round(Value, 3)), 
#               vjust = -0.3, size = 4, fontface = "bold") +
#     facet_wrap(~Metric, scales = "free_x", nrow = 2) +
#     scale_fill_manual(values = c("#2ECC71", "#E74C3C", "#3498DB")) +
#     labs(
#       title = "Model Performance by Metric",
#       subtitle = "Bayesian Logistic Regression for OCTA Prognosis",
#       x = "Model Type",
#       y = "Performance Score"
#     ) +
#     theme_bw() +
#     theme(
#       plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
#       plot.subtitle = element_text(hjust = 0.5, size = 12),
#       axis.title = element_text(size = 12, face = "bold"),
#       axis.text.x = element_text(angle = 45, hjust = 1),
#       strip.text = element_text(size = 11, face = "bold"),
#       strip.background = element_rect(fill = "lightgray", color = "white"),
#       legend.position = "none",
#       panel.grid.minor = element_blank()
#     ) +
#     ylim(0, 1.1)
#   
#   return(p)
# }
# 
# # æ–¹æ¡ˆ6: å­å¼¹å›¾ (Bullet Chart) - ç›®æ ‡å¯¹æ¯”å±•ç¤º
# create_bullet_performance <- function(model_performance) {
#   
#   # å‡†å¤‡æ•°æ®
#   perf_data <- data.frame()
#   
#   for(name in names(model_performance)) {
#     if(!is.null(model_performance[[name]])) {
#       perf_data <- rbind(perf_data, data.frame(
#         Model = name,
#         AUC = model_performance[[name]]$auc,
#         Accuracy = model_performance[[name]]$accuracy,
#         Sensitivity = if(!is.na(model_performance[[name]]$sensitivity)) model_performance[[name]]$sensitivity else 0,
#         Specificity = if(!is.na(model_performance[[name]]$specificity)) model_performance[[name]]$specificity else 0
#       ))
#     }
#   }
#   
#   # è½¬æ¢ä¸ºé•¿æ ¼å¼å¹¶è®¾ç½®ç›®æ ‡å€¼
#   perf_long <- perf_data %>%
#     pivot_longer(cols = c(AUC, Accuracy, Sensitivity, Specificity),
#                  names_to = "Metric", values_to = "Actual") %>%
#     mutate(
#       Target = 0.8,  # ç›®æ ‡æ€§èƒ½
#       Good = 0.7,    # è‰¯å¥½æ€§èƒ½
#       Metric_Model = paste(Metric, Model, sep = " â€¢ ")
#     )
#   
#   # åˆ›å»ºå­å¼¹å›¾
#   p <- ggplot(perf_long) +
#     # èƒŒæ™¯æ¡ï¼ˆç›®æ ‡ï¼‰
#     geom_col(aes(x = Metric_Model, y = Target), 
#              fill = "lightgray", alpha = 0.5, width = 0.8) +
#     # è‰¯å¥½æ€§èƒ½æ¡
#     geom_col(aes(x = Metric_Model, y = Good), 
#              fill = "gray", alpha = 0.7, width = 0.8) +
#     # å®é™…æ€§èƒ½æ¡
#     geom_col(aes(x = Metric_Model, y = Actual, fill = Metric), 
#              alpha = 0.9, width = 0.6) +
#     geom_text(aes(x = Metric_Model, y = Actual, label = round(Actual, 3)), 
#               hjust = -0.1, size = 3.5, fontface = "bold") +
#     coord_flip() +
#     scale_fill_manual(values = c("#2ECC71", "#3498DB", "#E74C3C", "#F39C12")) +
#     labs(
#       title = "Model Performance vs. Targets",
#       subtitle = "Gray = Good (0.7), Light Gray = Target (0.8), Colored = Actual",
#       x = "Model â€¢ Metric",
#       y = "Performance Score"
#     ) +
#     theme_minimal() +
#     theme(
#       plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
#       plot.subtitle = element_text(hjust = 0.5, size = 10),
#       axis.title = element_text(size = 12, face = "bold"),
#       legend.position = "bottom"
#     ) +
#     xlim(0, 1.1)
#   
#   return(p)
# }
# 
# # ================== ä½¿ç”¨ç¤ºä¾‹ ==================
# 
# # ç”Ÿæˆæ‰€æœ‰ç±»å‹çš„æ€§èƒ½å›¾è¡¨
# generate_all_performance_plots <- function(model_performance) {
#   
#   cat("===== ç”Ÿæˆå¤šç§æ€§èƒ½å±•ç¤ºå›¾è¡¨ =====\n")
#   
#   # 1. é›·è¾¾å›¾
#   tryCatch({
#     create_radar_performance(model_performance)
#   }, error = function(e) {
#     cat("é›·è¾¾å›¾ç”Ÿæˆå¤±è´¥:", e$message, "\n")
#   })
#   
#   # 2. çƒ­åŠ›å›¾
#   heatmap_plot <- create_heatmap_performance(model_performance)
#   if(!is.null(heatmap_plot)) {
#     ggsave("heatmap_performance.pdf", heatmap_plot, width = 10, height = 6)
#     cat("âœ“ çƒ­åŠ›å›¾å·²ä¿å­˜: heatmap_performance.pdf\n")
#   }
#   
#   # 3. æ°´å¹³æ¡å½¢å›¾
#   horizontal_plot <- create_horizontal_performance(model_performance)
#   if(!is.null(horizontal_plot)) {
#     ggsave("horizontal_performance.pdf", horizontal_plot, width = 12, height = 8)
#     cat("âœ“ æ°´å¹³æ¡å½¢å›¾å·²ä¿å­˜: horizontal_performance.pdf\n")
#   }
#   
#   # 4. åœ†ç¯å›¾
#   donut_plot <- create_donut_performance(model_performance, "Wearable Devices")
#   if(!is.null(donut_plot)) {
#     ggsave("donut_performance.pdf", donut_plot, width = 8, height = 8)
#     cat("âœ“ åœ†ç¯å›¾å·²ä¿å­˜: donut_performance.pdf\n")
#   }
#   
#   # 5. åˆ†é¢æ¡å½¢å›¾
#   faceted_plot <- create_faceted_performance(model_performance)
#   if(!is.null(faceted_plot)) {
#     ggsave("faceted_performance.pdf", faceted_plot, width = 12, height = 8)
#     cat("âœ“ åˆ†é¢å›¾å·²ä¿å­˜: faceted_performance.pdf\n")
#   }
#   
#   # 6. å­å¼¹å›¾
#   bullet_plot <- create_bullet_performance(model_performance)
#   if(!is.null(bullet_plot)) {
#     ggsave("bullet_performance.pdf", bullet_plot, width = 10, height = 8)
#     cat("âœ“ å­å¼¹å›¾å·²ä¿å­˜: bullet_performance.pdf\n")
#   }
#   
#   cat("\nğŸ¨ æ‰€æœ‰æ€§èƒ½å›¾è¡¨ç”Ÿæˆå®Œæˆï¼\n")
#   cat("è¯·é€‰æ‹©æ‚¨å–œæ¬¢çš„æ ·å¼ç”¨äºæ­£æ–‡ã€‚\n")
# }
# 
# all_plots <- generate_all_performance_plots(model_performance)



# ================== 15. æ•æ„Ÿæ€§åˆ†æï¼šæ’é™¤SH035æ ·æœ¬ ==================

cat("\n========================================================\n")
cat("===== æ•æ„Ÿæ€§åˆ†æï¼šæ’é™¤SH035æ ·æœ¬ =====\n")
cat("========================================================\n")

cat("ç›®çš„: éªŒè¯å¯ç©¿æˆ´è®¾å¤‡é¢„æµ‹æ¨¡å‹æ˜¯å¦ä¾èµ–äºç‰¹æ®Šæ‰‹æœ¯æ–¹å¼æ‚£è€…(PPV+IVI vs IVI only)\n")
cat("æ–¹æ³•: æ’é™¤SH035æ ·æœ¬ï¼Œé‡æ–°è®­ç»ƒå¯ç©¿æˆ´è®¾å¤‡æ¨¡å‹\n\n")

# ================== 15.1 æ•°æ®å‡†å¤‡ - æ’é™¤SH035 ==================

cat("===== æ•°æ®å‡†å¤‡ï¼šæ’é™¤SH035æ ·æœ¬ =====\n")

# æ£€æŸ¥SH035æ ·æœ¬æ˜¯å¦å­˜åœ¨
sh035_exists <- "SH035" %in% features_data_scaled$subject_id
cat("SH035æ ·æœ¬å­˜åœ¨:", ifelse(sh035_exists, "æ˜¯", "å¦"), "\n")

if(sh035_exists) {
  # æ‰¾åˆ°SH035æ ·æœ¬ä¿¡æ¯
  sh035_info <- features_data_scaled[features_data_scaled$subject_id == "SH035", ]
  cat("SH035æ ·æœ¬ä¿¡æ¯:\n")
  cat("- CV RHR:", round(sh035_info$cv_rhr, 3), "\n")
  cat("- Steps Max:", round(sh035_info$steps_max, 0), "\n")
  cat("- é¢„åç»“å±€:", ifelse(sh035_info$good_outcome == 1, "Good Outcome", "Poor Outcome"), "\n")
  
  # æ’é™¤SH035æ ·æœ¬
  features_data_sensitivity <- features_data_scaled[features_data_scaled$subject_id != "SH035", ]
  
  cat("\næ ·æœ¬å˜åŒ–:\n")
  cat("- åŸå§‹æ ·æœ¬æ•°:", nrow(features_data_scaled), "\n")
  cat("- æ•æ„Ÿæ€§åˆ†ææ ·æœ¬æ•°:", nrow(features_data_sensitivity), "\n")
  cat("- æ’é™¤æ ·æœ¬æ•°:", nrow(features_data_scaled) - nrow(features_data_sensitivity), "\n")
  
  # æ£€æŸ¥ç»“å±€åˆ†å¸ƒå˜åŒ–
  original_outcome_dist <- table(features_data_scaled$good_outcome)
  sensitivity_outcome_dist <- table(features_data_sensitivity$good_outcome)
  
  cat("\nç»“å±€åˆ†å¸ƒå˜åŒ–:\n")
  cat("åŸå§‹åˆ†å¸ƒ:\n")
  cat("- Poor Outcome (0):", original_outcome_dist[1], "(", round(original_outcome_dist[1]/sum(original_outcome_dist)*100, 1), "%)\n")
  cat("- Good Outcome (1):", original_outcome_dist[2], "(", round(original_outcome_dist[2]/sum(original_outcome_dist)*100, 1), "%)\n")
  
  cat("æ•æ„Ÿæ€§åˆ†æåˆ†å¸ƒ:\n")
  cat("- Poor Outcome (0):", sensitivity_outcome_dist[1], "(", round(sensitivity_outcome_dist[1]/sum(sensitivity_outcome_dist)*100, 1), "%)\n")
  cat("- Good Outcome (1):", sensitivity_outcome_dist[2], "(", round(sensitivity_outcome_dist[2]/sum(sensitivity_outcome_dist)*100, 1), "%)\n")
  
} else {
  cat("âŒ æœªæ‰¾åˆ°SH035æ ·æœ¬ï¼Œä½¿ç”¨åŸå§‹æ•°æ®é›†è¿›è¡Œæ•æ„Ÿæ€§åˆ†æ\n")
  features_data_sensitivity <- features_data_scaled
}

# ================== 15.2 æ•æ„Ÿæ€§åˆ†ææ¨¡å‹è®­ç»ƒ ==================

cat("\n===== æ•æ„Ÿæ€§åˆ†ææ¨¡å‹è®­ç»ƒ =====\n")

cat("è®­ç»ƒæ•æ„Ÿæ€§åˆ†ææ¨¡å‹ (æ’é™¤SH035)...\n")

# ä½¿ç”¨ç›¸åŒçš„è´å¶æ–¯è®¾ç½®
model_sensitivity <- stan_glm(
  good_outcome ~ cv_rhr_scaled + steps_max_scaled,
  data = features_data_sensitivity,
  family = binomial(link = "logit"),
  prior = prior_coef,
  prior_intercept = prior_intercept,
  chains = chains,
  iter = iter,
  warmup = warmup,
  seed = seed + 1,  # ç¨å¾®æ”¹å˜seedç¡®ä¿ç‹¬ç«‹æ€§
  cores = 4,
  refresh = 0
)

cat("âœ“ æ•æ„Ÿæ€§åˆ†ææ¨¡å‹è®­ç»ƒå®Œæˆ\n")

# ================== 15.3 æ¨¡å‹è¯Šæ–­å’Œæ”¶æ•›æ€§æ£€æŸ¥ ==================

cat("\n===== æ•æ„Ÿæ€§åˆ†ææ¨¡å‹è¯Šæ–­ =====\n")

# æ£€æŸ¥æ”¶æ•›æ€§
convergence_sensitivity <- check_convergence(model_sensitivity, "æ•æ„Ÿæ€§åˆ†ææ¨¡å‹")

# ================== 15.4 åéªŒåˆ†å¸ƒåˆ†æ ==================

cat("\n===== æ•æ„Ÿæ€§åˆ†æåéªŒåˆ†å¸ƒ =====\n")

# æå–åéªŒæ‘˜è¦
posterior_summary_sensitivity <- get_posterior_summary(model_sensitivity, "æ•æ„Ÿæ€§åˆ†ææ¨¡å‹")

# è¯¦ç»†å¯ä¿¡åŒºé—´åˆ†æ
analyze_credible_intervals(model_sensitivity, "æ•æ„Ÿæ€§åˆ†ææ¨¡å‹")

# ================== 15.5 é¢„æµ‹æ€§èƒ½è¯„ä¼° ==================

cat("\n===== æ•æ„Ÿæ€§åˆ†æé¢„æµ‹æ€§èƒ½ =====\n")

# è¯„ä¼°æ•æ„Ÿæ€§åˆ†ææ¨¡å‹æ€§èƒ½
performance_sensitivity <- evaluate_bayesian_model(
  model_sensitivity, 
  features_data_sensitivity, 
  "æ•æ„Ÿæ€§åˆ†ææ¨¡å‹"
)

# ================== 15.6 ä¸»åˆ†æ vs æ•æ„Ÿæ€§åˆ†ææ¯”è¾ƒ ==================

cat("\n===== ä¸»åˆ†æ vs æ•æ„Ÿæ€§åˆ†ææ¯”è¾ƒ =====\n")

# æ¯”è¾ƒåéªŒå‡å€¼
compare_posterior_estimates <- function(main_model, sensitivity_model) {
  cat("åéªŒå‡å€¼æ¯”è¾ƒ:\n")
  
  # æå–åéªŒæ ·æœ¬
  main_posterior <- as.matrix(main_model)
  sens_posterior <- as.matrix(sensitivity_model)
  
  # è·å–å‚æ•°å
  param_names <- colnames(main_posterior)
  coef_params <- param_names[!param_names %in% c("(Intercept)", "sigma")]
  
  comparison_results <- data.frame()
  
  for(param in coef_params) {
    main_mean <- mean(main_posterior[, param])
    main_ci <- quantile(main_posterior[, param], c(0.05, 0.95))
    
    sens_mean <- mean(sens_posterior[, param])
    sens_ci <- quantile(sens_posterior[, param], c(0.05, 0.95))
    
    difference <- sens_mean - main_mean
    
    cat(sprintf("\n%s:\n", param))
    cat(sprintf("  ä¸»åˆ†æ: %.3f [%.3f, %.3f]\n", main_mean, main_ci[1], main_ci[2]))
    cat(sprintf("  æ•æ„Ÿæ€§: %.3f [%.3f, %.3f]\n", sens_mean, sens_ci[1], sens_ci[2]))
    cat(sprintf("  å·®å¼‚: %.3f\n", difference))
    
    comparison_results <- rbind(comparison_results, data.frame(
      parameter = param,
      main_estimate = main_mean,
      main_ci_lower = main_ci[1],
      main_ci_upper = main_ci[2],
      sensitivity_estimate = sens_mean,
      sensitivity_ci_lower = sens_ci[1],
      sensitivity_ci_upper = sens_ci[2],
      difference = difference
    ))
  }
  
  return(comparison_results)
}

posterior_comparison <- compare_posterior_estimates(
  bayes_models[["Wearable Devices"]], 
  model_sensitivity
)

# æ¯”è¾ƒé¢„æµ‹æ€§èƒ½
compare_model_performance <- function(main_perf, sens_perf) {
  cat("\né¢„æµ‹æ€§èƒ½æ¯”è¾ƒ:\n")
  
  metrics <- c("auc", "accuracy", "sensitivity", "specificity")
  
  for(metric in metrics) {
    if(!is.null(main_perf[[metric]]) && !is.null(sens_perf[[metric]])) {
      main_value <- main_perf[[metric]]
      sens_value <- sens_perf[[metric]]
      difference <- sens_value - main_value
      
      cat(sprintf("%s:\n", toupper(metric)))
      cat(sprintf("  ä¸»åˆ†æ: %.3f\n", main_value))
      cat(sprintf("  æ•æ„Ÿæ€§: %.3f\n", sens_value))
      cat(sprintf("  å·®å¼‚: %+.3f\n", difference))
    }
  }
}

if(!is.null(performance_sensitivity)) {
  compare_model_performance(
    model_performance[["Wearable Devices"]], 
    performance_sensitivity
  )
}

# ================== 15.7 æ•æ„Ÿæ€§åˆ†æå¯è§†åŒ– ==================

cat("\n===== æ•æ„Ÿæ€§åˆ†æå¯è§†åŒ– =====\n")

# 1. åéªŒåˆ†å¸ƒå¯¹æ¯”å›¾
# ä¿®æ”¹æ•æ„Ÿæ€§åˆ†æå¯è§†åŒ– - ä½¿ç”¨mcmc_areaså‡½æ•°

create_posterior_comparison_plot <- function(main_model, sens_model, save_plot = TRUE) {
  
  # æå–åéªŒæ ·æœ¬
  main_posterior <- as.matrix(main_model)
  sens_posterior <- as.matrix(sens_model)
  
  # è·å–å‚æ•°å
  param_names <- colnames(main_posterior)
  coef_params <- param_names[!param_names %in% c("(Intercept)", "sigma")]
  
  if(length(coef_params) == 0) {
    cat("âŒ æ²¡æœ‰æ‰¾åˆ°ç³»æ•°å‚æ•°\n")
    return(NULL)
  }
  
  # 1. ä¸»åˆ†æåéªŒåˆ†å¸ƒå›¾ - ä½¿ç”¨mcmc_areas
  p1 <- mcmc_areas(main_posterior, 
                   pars = coef_params,
                   prob = 0.9) +
    ggtitle("Posterior Distributions with 90% Credible Intervals\nMain Analysis") +
    theme_bw() +
    theme(plot.title = element_text(hjust = 0.5, face = "bold"))
  
  # 2. æ•æ„Ÿæ€§åˆ†æåéªŒåˆ†å¸ƒå›¾ - ä½¿ç”¨mcmc_areas
  p2 <- mcmc_areas(sens_posterior, 
                   pars = coef_params,
                   prob = 0.9) +
    ggtitle("Posterior Distributions with 90% Credible Intervals\nSensitivity Analysis (Excluding SH035)") +
    theme_bw() +
    theme(plot.title = element_text(hjust = 0.5, face = "bold"))
  
  # 3. è½¨è¿¹å›¾å¯¹æ¯”
  p3 <- mcmc_trace(main_posterior, 
                   pars = coef_params) +
    ggtitle("MCMC Trace Plots\nMain Analysis") +
    theme_bw() +
    theme(plot.title = element_text(hjust = 0.5, face = "bold"))
  
  p4 <- mcmc_trace(sens_posterior, 
                   pars = coef_params) +
    ggtitle("MCMC Trace Plots\nSensitivity Analysis") +
    theme_bw() +
    theme(plot.title = element_text(hjust = 0.5, face = "bold"))
  
  # ç»„åˆå›¾ - 2x2å¸ƒå±€
  combined_plot <- grid.arrange(p1, p2, p3, p4, ncol = 2, nrow = 2)
  
  if(save_plot) {
    # ä¿å­˜å•ç‹¬çš„å›¾
    ggsave("sensitivity_main_posterior_areas.pdf", p1, width = 10, height = 6)
    ggsave("sensitivity_sens_posterior_areas.pdf", p2, width = 10, height = 6)
    ggsave("sensitivity_main_trace.pdf", p3, width = 10, height = 6)
    ggsave("sensitivity_sens_trace.pdf", p4, width = 10, height = 6)
    
    # ä¿å­˜ç»„åˆå›¾
    ggsave("sensitivity_posterior_comparison_mcmc.pdf", combined_plot, width = 16, height = 12)
    cat("âœ“ æ•æ„Ÿæ€§åˆ†æåéªŒåˆ†å¸ƒå›¾å·²ä¿å­˜ (ä½¿ç”¨mcmc_areas)\n")
  }
  
  return(list(main_areas = p1, sens_areas = p2, main_trace = p3, sens_trace = p4, combined = combined_plot))
}

# åœ¨æ‚¨çš„å‡½æ•°å®šä¹‰åé¢æ·»åŠ è¿™è¡Œä»£ç æ¥è°ƒç”¨å‡½æ•°ï¼š
sensitivity_plots <- create_posterior_comparison_plot(
  bayes_models[["Wearable Devices"]], 
  model_sensitivity
)

# 2. æ€§èƒ½å¯¹æ¯”å›¾
create_sensitivity_performance_plot <- function(main_perf, sens_perf, save_plot = TRUE) {
  
  if(is.null(sens_perf)) {
    cat("âŒ æ•æ„Ÿæ€§åˆ†ææ€§èƒ½æ•°æ®ä¸å¯ç”¨\n")
    return(NULL)
  }
  
  # å‡†å¤‡æ€§èƒ½å¯¹æ¯”æ•°æ®
  perf_comparison <- data.frame(
    Metric = c("AUC", "Accuracy", "Sensitivity", "Specificity"),
    Main_Analysis = c(main_perf$auc, main_perf$accuracy, 
                      main_perf$sensitivity, main_perf$specificity),
    Sensitivity_Analysis = c(sens_perf$auc, sens_perf$accuracy,
                             sens_perf$sensitivity, sens_perf$specificity)
  )
  
  # è®¡ç®—å·®å¼‚
  perf_comparison$Difference <- perf_comparison$Sensitivity_Analysis - perf_comparison$Main_Analysis
  
  # è½¬æ¢ä¸ºé•¿æ ¼å¼
  perf_long <- perf_comparison %>%
    pivot_longer(cols = c(Main_Analysis, Sensitivity_Analysis),
                 names_to = "Analysis", values_to = "Value") %>%
    mutate(
      Analysis = factor(Analysis, levels = c("Main_Analysis", "Sensitivity_Analysis"),
                        labels = c("Main Analysis", "Sensitivity Analysis")),
      Metric = factor(Metric, levels = c("AUC", "Accuracy", "Sensitivity", "Specificity"))
    )
  
  # åˆ›å»ºå¯¹æ¯”å›¾
  p1 <- ggplot(perf_long, aes(x = Metric, y = Value, fill = Analysis)) +
    geom_col(position = "dodge", alpha = 0.8, width = 0.7) +
    geom_text(aes(label = round(Value, 3)), 
              position = position_dodge(width = 0.7), 
              vjust = -0.3, size = 3.5, fontface = "bold") +
    scale_fill_manual(values = c("Main Analysis" = "#2C3E50", 
                                 "Sensitivity Analysis" = "#E74C3C")) +
    labs(
      title = "Performance Comparison: Main vs Sensitivity Analysis",
      subtitle = "Sensitivity Analysis: Excluding SH035 Sample",
      x = "Performance Metric", 
      y = "Performance Score", 
      fill = "Analysis Type"
    ) +
    theme_bw() +
    theme(
      plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
      plot.subtitle = element_text(hjust = 0.5, size = 11),
      legend.position = "bottom"
    ) +
    ylim(0, 1.1)
  
  # åˆ›å»ºå·®å¼‚å›¾
  diff_data <- perf_comparison[, c("Metric", "Difference")]
  
  p2 <- ggplot(diff_data, aes(x = Metric, y = Difference)) +
    geom_col(fill = "#F39C12", alpha = 0.8, width = 0.6) +
    geom_text(aes(label = sprintf("%+.3f", Difference)), 
              vjust = ifelse(diff_data$Difference >= 0, -0.3, 1.3), 
              size = 4, fontface = "bold") +
    geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.7) +
    labs(
      title = "Performance Difference (Sensitivity - Main)",
      subtitle = "Positive values indicate improvement in sensitivity analysis",
      x = "Performance Metric",
      y = "Difference in Performance"
    ) +
    theme_bw() +
    theme(
      plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
      plot.subtitle = element_text(hjust = 0.5, size = 11)
    )
  
  # ç»„åˆå›¾
  combined_perf_plot <- grid.arrange(p1, p2, ncol = 1, nrow = 2)
  
  if(save_plot) {
    ggsave("sensitivity_performance_comparison.pdf", p1, width = 10, height = 6)
    ggsave("sensitivity_performance_difference.pdf", p2, width = 10, height = 6)
    ggsave("sensitivity_performance_combined.pdf", combined_perf_plot, width = 12, height = 10)
    cat("âœ“ æ•æ„Ÿæ€§åˆ†ææ€§èƒ½å¯¹æ¯”å›¾å·²ä¿å­˜\n")
  }
  
  return(list(comparison = p1, difference = p2, combined = combined_perf_plot))
}

if(!is.null(performance_sensitivity)) {
  performance_plots_sensitivity <- create_sensitivity_performance_plot(
    model_performance[["Wearable Devices"]], 
    performance_sensitivity
  )
}

# 3. ROCæ›²çº¿å¯¹æ¯”å›¾
create_sensitivity_roc_comparison <- function(main_perf, sens_perf, save_plot = TRUE) {
  
  if(is.null(sens_perf) || is.null(sens_perf$roc_obj)) {
    cat("âŒ æ•æ„Ÿæ€§åˆ†æROCæ•°æ®ä¸å¯ç”¨\n")
    return(NULL)
  }
  
  # å‡†å¤‡ROCæ•°æ®
  main_roc <- main_perf$roc_obj
  sens_roc <- sens_perf$roc_obj
  
  roc_data <- rbind(
    data.frame(
      sensitivity = main_roc$sensitivities,
      specificity = main_roc$specificities,
      fpr = 1 - main_roc$specificities,
      analysis = "Main Analysis",
      auc = main_perf$auc
    ),
    data.frame(
      sensitivity = sens_roc$sensitivities,
      specificity = sens_roc$specificities,
      fpr = 1 - sens_roc$specificities,
      analysis = "Sensitivity Analysis",
      auc = sens_perf$auc
    )
  )
  
  # åˆ›å»ºåˆ†ææ ‡ç­¾
  roc_data$label <- paste0(roc_data$analysis, " (AUC = ", round(roc_data$auc, 3), ")")
  
  # åˆ›å»ºROCå¯¹æ¯”å›¾
  roc_plot <- ggplot(roc_data, aes(x = fpr, y = sensitivity, color = label)) +
    geom_line(size = 1.5, alpha = 0.8) +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray50", alpha = 0.7) +
    scale_color_manual(values = c(
      paste0("Main Analysis (AUC = ", round(main_perf$auc, 3), ")") = "#2C3E50",
      paste0("Sensitivity Analysis (AUC = ", round(sens_perf$auc, 3), ")") = "#E74C3C"
    )) +
    labs(
      title = "ROC Curves: Main vs Sensitivity Analysis",
      subtitle = "Sensitivity Analysis: Excluding SH035 Sample",
      x = "False Positive Rate (1 - Specificity)",
      y = "True Positive Rate (Sensitivity)",
      color = "Analysis Performance"
    ) +
    theme_bw() +
    theme(
      plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
      plot.subtitle = element_text(hjust = 0.5, size = 11),
      legend.position = "bottom"
    ) +
    coord_fixed() +
    xlim(0, 1) + ylim(0, 1)
  
  if(save_plot) {
    ggsave("sensitivity_roc_comparison.pdf", roc_plot, width = 10, height = 8)
    cat("âœ“ æ•æ„Ÿæ€§åˆ†æROCå¯¹æ¯”å›¾å·²ä¿å­˜\n")
  }
  
  return(roc_plot)
}

if(!is.null(performance_sensitivity)) {
  roc_plot_sensitivity <- create_sensitivity_roc_comparison(
    model_performance[["Wearable Devices"]], 
    performance_sensitivity
  )
}

# ================== 15.8 ç¨³å¥æ€§è¯„ä¼° ==================

cat("\n===== ç¨³å¥æ€§è¯„ä¼° =====\n")

# è¯„ä¼°æ¨¡å‹ç¨³å¥æ€§
assess_model_robustness <- function(main_perf, sens_perf, posterior_comparison) {
  
  cat("æ¨¡å‹ç¨³å¥æ€§è¯„ä¼°:\n\n")
  
  # 1. é¢„æµ‹æ€§èƒ½ç¨³å¥æ€§
  if(!is.null(sens_perf)) {
    auc_diff <- abs(sens_perf$auc - main_perf$auc)
    acc_diff <- abs(sens_perf$accuracy - main_perf$accuracy)
    
    cat("1. é¢„æµ‹æ€§èƒ½ç¨³å¥æ€§:\n")
    cat(sprintf("   - AUCå·®å¼‚: %.3f\n", auc_diff))
    cat(sprintf("   - å‡†ç¡®ç‡å·®å¼‚: %.3f\n", acc_diff))
    
    # åˆ¤æ–­ç¨³å¥æ€§
    auc_robust <- auc_diff < 0.05
    acc_robust <- acc_diff < 0.05
    
    cat(sprintf("   - AUCç¨³å¥æ€§: %s\n", ifelse(auc_robust, "âœ“ ç¨³å¥", "âŒ ä¸ç¨³å¥")))
    cat(sprintf("   - å‡†ç¡®ç‡ç¨³å¥æ€§: %s\n", ifelse(acc_robust, "âœ“ ç¨³å¥", "âŒ ä¸ç¨³å¥")))
  }
  
  # 2. å‚æ•°ä¼°è®¡ç¨³å¥æ€§
  cat("\n2. å‚æ•°ä¼°è®¡ç¨³å¥æ€§:\n")
  
  for(i in 1:nrow(posterior_comparison)) {
    param <- posterior_comparison$parameter[i]
    diff <- abs(posterior_comparison$difference[i])
    
    # åˆ¤æ–­ç¨³å¥æ€§ï¼ˆå·®å¼‚å°äº0.2è¢«è®¤ä¸ºæ˜¯ç¨³å¥çš„ï¼‰
    robust <- diff < 0.2
    
    cat(sprintf("   - %s: å·®å¼‚=%.3f %s\n", 
                param, diff, ifelse(robust, "âœ“ ç¨³å¥", "âŒ ä¸ç¨³å¥")))
  }
  
  # 3. å¯ä¿¡åŒºé—´é‡å è¯„ä¼°
  cat("\n3. å¯ä¿¡åŒºé—´é‡å è¯„ä¼°:\n")
  
  for(i in 1:nrow(posterior_comparison)) {
    param <- posterior_comparison$parameter[i]
    
    # æ£€æŸ¥90%å¯ä¿¡åŒºé—´æ˜¯å¦é‡å 
    main_lower <- posterior_comparison$main_ci_lower[i]
    main_upper <- posterior_comparison$main_ci_upper[i]
    sens_lower <- posterior_comparison$sensitivity_ci_lower[i]
    sens_upper <- posterior_comparison$sensitivity_ci_upper[i]
    
    overlap <- max(main_lower, sens_lower) <= min(main_upper, sens_upper)
    
    cat(sprintf("   - %s: %s\n", param, ifelse(overlap, "âœ“ åŒºé—´é‡å ", "âŒ åŒºé—´ä¸é‡å ")))
  }
  
  # æ€»ä½“ç¨³å¥æ€§è¯„ä¼°
  cat("\n4. æ€»ä½“ç¨³å¥æ€§ç»“è®º:\n")
  
  if(!is.null(sens_perf)) {
    overall_robust <- (auc_diff < 0.05) && (acc_diff < 0.05) && 
      all(abs(posterior_comparison$difference) < 0.2)
    
    cat(sprintf("   - æ•´ä½“è¯„ä¼°: %s\n", 
                ifelse(overall_robust, "âœ“ æ¨¡å‹ç¨³å¥", "âš ï¸ æ¨¡å‹æ•æ„Ÿ")))
    
    if(overall_robust) {
      cat("   - ç»“è®º: æ’é™¤SH035æ ·æœ¬åï¼Œæ¨¡å‹æ€§èƒ½å’Œå‚æ•°ä¼°è®¡ä¿æŒç¨³å®š\n")
      cat("   - ä¸´åºŠæ„ä¹‰: æ¨¡å‹ä¸ä¾èµ–äºç‰¹æ®Šæ‰‹æœ¯æ–¹å¼æ‚£è€…ï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›\n")
    } else {
      cat("   - ç»“è®º: æ’é™¤SH035æ ·æœ¬å¯¹æ¨¡å‹æœ‰æ˜¾è‘—å½±å“\n")
      cat("   - ä¸´åºŠæ„ä¹‰: éœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥SH035æ ·æœ¬çš„ç‰¹æ®Šæ€§\n")
    }
  }
}

# æ‰§è¡Œç¨³å¥æ€§è¯„ä¼°
if(!is.null(performance_sensitivity)) {
  assess_model_robustness(
    model_performance[["Wearable Devices"]], 
    performance_sensitivity, 
    posterior_comparison
  )
}

# ================== 15.9 ä¿å­˜æ•æ„Ÿæ€§åˆ†æç»“æœ ==================

cat("\n===== ä¿å­˜æ•æ„Ÿæ€§åˆ†æç»“æœ =====\n")

# ä¿å­˜æ•æ„Ÿæ€§åˆ†ææ¨¡å‹
saveRDS(model_sensitivity, "sensitivity_analysis_model_exclude_sh035.rds")

# ä¿å­˜æ•æ„Ÿæ€§åˆ†ææ€§èƒ½
saveRDS(performance_sensitivity, "sensitivity_analysis_performance.rds")

# ä¿å­˜åéªŒæ¯”è¾ƒç»“æœ
saveRDS(posterior_comparison, "sensitivity_posterior_comparison.rds")

# ä¿å­˜æ”¶æ•›æ€§ç»“æœ
saveRDS(convergence_sensitivity, "sensitivity_convergence_diagnostics.rds")

# ================== 15.10 æ•æ„Ÿæ€§åˆ†ææŠ¥å‘Š ==================

cat("\n===== ç”Ÿæˆæ•æ„Ÿæ€§åˆ†ææŠ¥å‘Š =====\n")

generate_sensitivity_report <- function(main_perf, sens_perf, posterior_comp, original_n, sensitivity_n) {
  
  report <- paste0(
    "========================================================\n",
    "SENSITIVITY ANALYSIS REPORT\n",
    "Excluding SH035 Sample: PPV+IVI vs IVI Only Validation\n",
    "========================================================\n\n",
    
    "OBJECTIVE\n",
    "Assess whether the wearable device prediction model depends on\n",
    "patients with special surgical procedures (PPV+IVI vs IVI only)\n",
    "by excluding the SH035 sample from the analysis.\n\n",
    
    "METHODOLOGY\n",
    "- Exclusion Criteria: SH035 sample removed\n",
    "- Model Type: Bayesian logistic regression\n",
    "- Features: CV RHR + Steps Max (Late Recovery period)\n",
    "- Comparison: Main analysis vs Sensitivity analysis\n\n",
    
    "SAMPLE CHANGES\n",
    "- Original Sample Size: ", original_n, "\n",
    "- Sensitivity Analysis Size: ", sensitivity_n, "\n",
    "- Excluded Samples: ", original_n - sensitivity_n, "\n\n"
  )
  
  if(!is.null(sens_perf) && !is.null(main_perf)) {
    auc_diff <- sens_perf$auc - main_perf$auc
    acc_diff <- sens_perf$accuracy - main_perf$accuracy
    
    report <- paste0(report,
                     "PERFORMANCE COMPARISON\n",
                     sprintf("Main Analysis AUC: %.3f\n", main_perf$auc),
                     sprintf("Sensitivity Analysis AUC: %.3f\n", sens_perf$auc),
                     sprintf("AUC Difference: %+.3f\n", auc_diff),
                     sprintf("Accuracy Difference: %+.3f\n\n", acc_diff))
  }
  
  # æ·»åŠ å‚æ•°æ¯”è¾ƒ
  report <- paste0(report, "PARAMETER COMPARISON\n")
  for(i in 1:nrow(posterior_comp)) {
    param <- posterior_comp$parameter[i]
    main_est <- posterior_comp$main_estimate[i]
    sens_est <- posterior_comp$sensitivity_estimate[i]
    diff <- posterior_comp$difference[i]
    
    report <- paste0(report,
                     sprintf("%s:\n", param),
                     sprintf("  Main: %.3f, Sensitivity: %.3f, Diff: %+.3f\n", 
                             main_est, sens_est, diff))
  }
  
  # ç¨³å¥æ€§ç»“è®º
  if(!is.null(sens_perf) && !is.null(main_perf)) {
    auc_robust <- abs(auc_diff) < 0.05
    param_robust <- all(abs(posterior_comp$difference) < 0.2)
    overall_robust <- auc_robust && param_robust
    
    report <- paste0(report, "\n",
                     "ROBUSTNESS ASSESSMENT\n",
                     sprintf("AUC Stability: %s (|diff| = %.3f)\n", 
                             ifelse(auc_robust, "ROBUST", "SENSITIVE"), abs(auc_diff)),
                     sprintf("Parameter Stability: %s\n", 
                             ifelse(param_robust, "ROBUST", "SENSITIVE")),
                     sprintf("Overall Assessment: %s\n\n", 
                             ifelse(overall_robust, "MODEL ROBUST", "MODEL SENSITIVE")))
    
    if(overall_robust) {
      report <- paste0(report,
                       "CLINICAL IMPLICATIONS\n",
                       "1. Model performance remains stable after excluding SH035\n",
                       "2. Wearable device predictions are not dependent on special\n",
                       "   surgical procedure patients (PPV+IVI vs IVI only)\n",
                       "3. Model demonstrates good generalizability across\n",
                       "   different surgical approaches\n",
                       "4. CV RHR and Steps Max maintain consistent predictive value\n",
                       "5. Findings support broader clinical implementation\n\n")
    } else {
      report <- paste0(report,
                       "CLINICAL IMPLICATIONS\n",
                       "1. Model shows sensitivity to SH035 sample exclusion\n",
                       "2. Further investigation needed for SH035 characteristics\n",
                       "3. Potential surgical procedure-specific effects\n",
                       "4. Model validation in larger, diverse samples recommended\n",
                       "5. Caution advised for clinical implementation\n\n")
    }
  }
  
  report <- paste0(report,
                   "CONCLUSION\n",
                   "This sensitivity analysis evaluated the robustness of the\n",
                   "wearable device prediction model by excluding the SH035 sample\n",
                   "to test for dependency on special surgical procedures. ",
                   if(!is.null(sens_perf) && abs(sens_perf$auc - main_perf$auc) < 0.05) {
                     "The\nmodel demonstrated robust performance with minimal changes\nin predictive accuracy and parameter estimates."
                   } else {
                     "The\nanalysis revealed some sensitivity to sample composition,\nsuggesting the need for larger validation studies."
                   },
                   "\n\n",
                   "Generated: ", Sys.time(), "\n",
                   "========================================================"
  )
  
  return(report)
}

# ç”Ÿæˆå¹¶ä¿å­˜æ•æ„Ÿæ€§åˆ†ææŠ¥å‘Š
if(!is.null(performance_sensitivity)) {
  sensitivity_report <- generate_sensitivity_report(
    model_performance[["Wearable Devices"]], 
    performance_sensitivity, 
    posterior_comparison,
    nrow(features_data_scaled),
    nrow(features_data_sensitivity)
  )
  
  writeLines(sensitivity_report, "sensitivity_analysis_report_exclude_sh035.txt")
  cat("âœ“ æ•æ„Ÿæ€§åˆ†ææŠ¥å‘Šå·²ä¿å­˜\n")
}

# ================== 15.11 å®Œæˆæ€»ç»“ ==================

cat("\n===== æ•æ„Ÿæ€§åˆ†æå®Œæˆæ€»ç»“ =====\n")

cat("ç”Ÿæˆçš„æ•æ„Ÿæ€§åˆ†ææ–‡ä»¶:\n")
sensitivity_files <- c(
  "sensitivity_analysis_model_exclude_sh035.rds",
  "sensitivity_analysis_performance.rds",
  "sensitivity_posterior_comparison.rds",
  "sensitivity_convergence_diagnostics.rds",
  "sensitivity_analysis_report_exclude_sh035.txt"
)

for(i in seq_along(sensitivity_files)) {
  if(file.exists(sensitivity_files[i])) {
    cat(sprintf("âœ“ %d. %s\n", i, sensitivity_files[i]))
  }
}

# æ£€æŸ¥æ•æ„Ÿæ€§åˆ†æPDFå›¾è¡¨æ–‡ä»¶
sensitivity_pdfs <- list.files(pattern = "sensitivity.*\\.pdf$")
if(length(sensitivity_pdfs) > 0) {
  cat("\nç”Ÿæˆçš„æ•æ„Ÿæ€§åˆ†æå›¾è¡¨:\n")
  for(i in seq_along(sensitivity_pdfs)) {
    cat(sprintf("âœ“ %d. %s\n", i, sensitivity_pdfs[i]))
  }
}

# æœ€ç»ˆæ•æ„Ÿæ€§åˆ†ææ€»ç»“
cat("\nğŸ¯ æ•æ„Ÿæ€§åˆ†ææ ¸å¿ƒå‘ç°:\n")

if(!is.null(performance_sensitivity)) {
  main_auc <- model_performance[["Wearable Devices"]]$auc
  sens_auc <- performance_sensitivity$auc
  auc_change <- sens_auc - main_auc
  
  cat(sprintf("AUCå˜åŒ–: %.3f â†’ %.3f (å·®å¼‚: %+.3f)\n", main_auc, sens_auc, auc_change))
  
  if(abs(auc_change) < 0.05) {
    cat("âœ… æ¨¡å‹ç¨³å¥æ€§: ä¼˜ç§€ (AUCå˜åŒ– < 0.05)\n")
    cat("ğŸ“‹ ä¸´åºŠç»“è®º: æ¨¡å‹ä¸ä¾èµ–ç‰¹æ®Šæ‰‹æœ¯æ–¹å¼æ‚£è€…\n")
    cat("ğŸ¥ åº”ç”¨ä»·å€¼: æ”¯æŒæ›´å¹¿æ³›çš„ä¸´åºŠå®æ–½\n")
  } else {
    cat("âš ï¸ æ¨¡å‹ç¨³å¥æ€§: ä¸­ç­‰ (AUCå˜åŒ– â‰¥ 0.05)\n")
    cat("ğŸ“‹ ä¸´åºŠç»“è®º: éœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥SH035æ ·æœ¬ç‰¹å¾\n")
    cat("ğŸ¥ åº”ç”¨ä»·å€¼: å»ºè®®æ‰©å¤§æ ·æœ¬éªŒè¯\n")
  }
}

cat("\nâœ… æ•æ„Ÿæ€§åˆ†æå®Œæˆï¼\n")
cat("ğŸ“Š æ ¸å¿ƒä»·å€¼: éªŒè¯å¯ç©¿æˆ´è®¾å¤‡æ¨¡å‹å¯¹ä¸åŒæ‰‹æœ¯æ–¹å¼çš„æ³›åŒ–èƒ½åŠ›\n")
cat("ğŸ”¬ ç§‘å­¦æ„ä¹‰: æ’é™¤ç‰¹æ®Šæ ·æœ¬çš„å½±å“ï¼Œå¢å¼ºç»“æœå¯ä¿¡åº¦\n")

cat("\n========================================================\n")
cat("===== å…¨éƒ¨åˆ†æå®Œæˆ (ä¸»åˆ†æ + æ•æ„Ÿæ€§åˆ†æ) =====\n")
cat("========================================================\n")

